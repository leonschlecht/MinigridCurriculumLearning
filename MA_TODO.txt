eval "$(~/tmp/Anaconda/home/schlecht/AnacondaNeu/bin/conda shell.bash hook)"

@alex:
- dieses neue paper erwähnen ?
- quellen nicht nur hinrotzen (--> erfordert dann iwi immer mehr zeit als gedacht; weil ich es ja ordenltich machen will)
- Training time: sonst vergleichen die leute ja immer nur epochs / iterations & nicht sowas wie RH dass man quasi wasted simulations hat ?
- Wie oft zitieren? Also manchmal ergibt es sich im absatz durch They usw ja von selbst ?
- Wie erkläre ich das mit TSCL? / Warum ich das nicht benutzt habe
- "Awesome CL" git repo
- - PPO failt 16x16 in diesem paper auch https://arxiv.org/pdf/2304.10770.pdf ; habe ja auch ein paar runs mit anderen LRs ---> Oder muss ich dann erklären, wie die damit umgeganegn sind ??
- wie mit dem ablation stduy erwähnen? Habe ja zB 1 gen getestet (vs Rnd RH); oder 1 curric oder 1 step


------------


- TODO für den plot
    - y achse wieder sortieren (achtung dass die values auch mitgemappt werden)
    - farben fixen
    - größe der punkte
    - überlegen,wie man das gut plotten kann
    - mach die y achse näher beinander
    - ggf etws anderes als kreise zum darstellen benutzen
    - bei 2.2 die markierung reinpacken (oder sogar basieren auf den iterSteps, für exakten marker)
        Achsen usw noch anpassen (beschränkung y)
        farbe hinbekommen (hue mus gesettz sein)
        4 nebeneinander noch machen
        vllt auch mal schreiben morgen
        oder mind. noch die neuen experimente Queuen mit der anderen umgebung !!!
- Die x-tick labels einheitlich machen (zB Wan nsoll _NSGA dazukommen,



PLOT todo :
- einen nsga&ga filter einbauen
- Überall titel hinzufügen mit einheitlicher schriftgröße
- Legend immer hinzufügen + fontsize anpassen
- unnötigen krams aus legend rauscutten (also zb "Column" oder das n bei "6x6n")
- Größere x & y achsenbeschriftungen
- Bessere Gruppierungsmöglichkeiten einbauen, ggf auch mal subplots verwenden.
- Die x-tick labels einheitlich machen (zB Wan nsoll _NSGA dazukommen,
- Manche Ordner besser benennen (zB _step anpassen an das was ich eigentlich benutze)



- Kann ich berechnen wie viele komplette passes durch die env meine Frames haben? (5kk / 1440 ?), oder vllt mit der Distribution verrechnen41
- # Is it relevant how the curricula REWARDS look like at later generations ? We could compare it against the snapshot and notice differences i guess
- Ne Grafik, die zeigt, dass Gamma relevant ist und was sonst passieren könnte (Alle Curric Gleichgewichtet -> Schlechter snapshot gefahr
- Vllt viel weniger iterationen machen, aber dafür viel geringere steps anpeilen (müsste man dann evtl mit PPO anpassen, damit er nicht übershootet)
- limitation: ineperience des researchers
- Wenn ein gutes model auf 6x6 trainiert, wird es dann schlechter in 10x10 & 12x12 ?


- limitation: poor understanding of the actual leraning curve still, because minigrid too easy potentially
- limitation maybe the max step is useless and just confusing for the results ( --> longer training time especially)
- Why does no RS have more usage of 12x12 ??
- Kann man spcl nicht mit dem para env abusen ? Also dass man 8 steps hat statt 4 (
    6666
    6688
    8888
    88zz
    zzzz
    zztt
    tttt
- sollte ich runs nicht vllt 10x durchlaufen lasesn ?
- Curriculum Progression plot evtl für die ersten 1kk
============


- Evaluate überarbeiten (evtl neuschreiben --> curricScore vs snapshot -- Warum unterschiedlich ???) (evtl liegt es an alten logs an was anderem)
    weil eig sollte der snapshot ja aus dem best curric abgeleitet werden können
    - Vllt mal snapshot anders probieren & schauen ob es dadurch unterschiede gibt
    - Ich glaube die snapshotscore plots sind einfach broken (minimal), aber nicht schlimm. Der ALgorithmus hat schon richtig gehandelt


Samstag:
- Alle neuen Runs Queuen (und die Configuration datei damit füllen, und Tabelle updaten)
    Dynamic obstacle: --noRewardShaping
        - Q RHEA CL (3 scripts - seed) 5x queued
        - TODO fix & test wegen rewrads log ; RRH (3 scripts - seed): CurriculumRewards (1 eintrag bei 2numCurric); rawRewards fehlt
        - Q PPOnly (4 Scripts - envChange)
        - Q AP normal (1 script)
        - Q AP asCurric (1 script)
        - Q SPCL (1 script)
    DKey:
        - Q PPO andere LR
        - Q RHEA CL 1 paraEnv
        constMaxsteps:
            - Q PPO andere LR (1 script)
            - Q RHEA const maxsteps (3-9 scripts)
            - TODO RRH (3 scripts)
            - Q SPCL (1 script)
            - Q AP normal (1 script)
            - Q AP as curric (1 script) [queued mit Dynamic obstacle ausvereshen)
        Q sobol: mehr Runs queuen damit es einheitlich ist bzgl der nRS & RS Parameter



----------------

- Andere Env als Dynamic Obstacle oder nicht ??
- Bessere RHEA CL ModelName generierung ? Damit ich das nicht immre alles manuell machen muss ...
(d) --> geht nicht wirklich - testen ob ich die .sh scripts in einen ordner packen kann, damit der root ordner nicht so voll ist ( --> testen ob python .py statt -m geht)
- limitation: desired effekt von "langsamer anstieg, aber dann besser" leider nicht beobachtet ( war ja auch die Hoffnung von gamma)
- Sind die nRS > RS oder tritt das nicht immer auf???
- Evaluate so überarbeiten dass ich 1 df habe und nicht 20 listen







--------------------------------------------------------------------------------------------------------

iwann bald:
- die 4x1 plots nebeneinander mit dem neuen reward
- mal einen sanity check machen (neuen Run?) wegen den max steps (ob das ein problem war oder nicht, weil es ja schon inconsisten wäre sonst)
- selectedEnvs plot weiterhinbekommen. GGf auch als 1x2 subplot (Aber wie kann ich 5x runs damit plotten --> ggf majority vote?)
!! - diese RHEA CL Hoffnung mal irgendwie als graifk hinbekommen , vllt kann ich das für ja auch existierende logs benutzen
- Allg plot todos überarbeiten (S.u. "todofür den plot")
- aus welcher gen kommt das beste curriculum - interessant?
- all Curric plot useful? Kann man evtl besser zeigen ok: best curric sieht die sitr so aus, 1. step sieht sie so aus ...
- wenn jetzt multiobj, hätte ich nicht auch nsga 3 benutzen können / sollen ? --> Wie erkläre ich das
- RW: Einen RHEA CL Ansatz finden ??
- SAC oder TD3 noch mal aufnehmen
- Überlegen, welche Gruppierungen ich brauche für die asuwertung
- Feedback noch mal drüberschaue
- einen nsga&ga filter einbauen
- Überall titel hinzufügen mit einheitlicher schriftgröße
- Legend immer hinzufügen + fontsize anpassen
- unnötigen krams aus legend rauscutten (also zb "Column" oder das n bei "6x6n")
- Größere x & y achsenbeschriftungen
- Bessere Gruppierungsmöglichkeiten einbauen, ggf auch mal subplots verwenden.
- Mal ausprobieren, ob ich beim weitertrainieren von bestimmten Modeln (auf 10x10 auch stagniere)
- 12x12 ist nie in den snapshots mit drin --> iwi für future work / limitations mit aufnehmen, weil man das ja langfristig iwi ermutigen möchte (wenn es immer im 3. schritt attraktiv ist aber nie im 1. und sich dann jede epoch wiederholen könnte vllt
    aber irgendwie ist diese begründung bisher mit dem reward shaping entfernen auch nicht so gut. Bzw hat das überhaupt was gebracht ?
- PLOT aus welcher gen kam das beste curriculum (vllt kann man das irgendwie ranken oder so?) & dann auch für alle irgendwie kombinieren (von einer exp configuration)
- Future work: ablation study --> Wie würde das überhaupt aussehen ?
- baseline: manual curriculum (ggf auch in AP einbauen?)
- limitation: zu wenig zeit mit der architektur verbracht & den PPO hypers
- limitation: zu wenig trainignszeit um besser in 12x12 zu werden? Müsste ggf erst öfter trainieren, um den positiven effekt zu sehen (bei sowas ist TSCL gluabe ich einfach besser, wenn man abs(slope) betrachtet)
- Wie kann es sein dass er erst besser wird im 12x12 und dann iwann alles wieder verlernt ???

-Gamma50 hat schnellsten anstieg (sollte auch so sein, weil greedieste variante); Aber warum hat gamma70 slightly worse am ende ?
- NSGA multi vs single: Multi scheint 1% besser zu sein (oderi st es wegen nRS ?)
- Ist das PPO Netzwerk zu schwach?
- Ist dieser eine Run mit "newLogs" durch ??
- Limitation: wie umgehen mit catastrophic forgetting? TSCL macht das viel besser mit dem abs(slope) dass man auch informationen aus dervergangenheit verwendet



- Kapitel 2 überarbeiten (Grafiken einbinden (zB wang et al obj func), verlinkungen machen, non-convex kram erwähnen; Minigrid zuende ; NSGA
- experimente ohne MAXSTEPS anpassung machen (--constMaxsteps)
- curricuulumScore wird als Formel eingeführt, taucht aber nicht im pseudocode auf ?


evtl:
- Docstirngs anfangen zu schreiben
- Experimente evtl mal richtig benenen dass man sie besser einordnen kann (legend ist confusing sonst) [oder im cod transformieren]
- Wie EA Initialisieren, wenn man train != eval envs. man könnte evtl. mehr envs zum Training verwenden könnte, als man in der Evaluierung benutzt, um Kosten zu sparen (zB TrainingsEnv: 6,8,10,12,14,16 und evaluierung mit 6, 8, 12, 16. Wie würde sich der EA dadurch verändern? Für single obj macht es glaube ich keinen Unterschied, aber für Multi Obj?
- AP aufteilen als oberklasse und 4 unterklassen

---------------------------------------



Allg:
- EAs und anderen Envs. Ich hatte ja diesen Gedanken, dass man evtl. mehr envs zum Training verwenden könnte, als man in der Evaluierung benutzt, um Kosten zu sparen (zB TrainingsEnv: 6,8,10,12,14,16 und evaluierung mit 6, 8, 12, 16. Wie würde sich der EA dadurch verändern? Für single obj macht es glaube ich keinen Unterschied, aber für Multi Obj?
- war das schlimm mit den maxSteps ??? Dadurch dass man überall die 12x12 genommen hat.
    und auch prüfen, ob das für andere experimente auch genommen wurde



low prio
- Mal ausprobieren, ob ich beim weitertrainieren von bestimmten Modeln (auf 10x10 auch stagniere) [bzw auf spcl.15 konnte er es ja hatlen]
- überlegen: Ist CrossoverMutation 80/80 optimal für DoorKey?
- @Minigrid: Training a PPO Agent ??? --> probier das mal aus i guess


-----------------------------------------------------------------

SAC todo:
    schauen wie ich das speichern kann
    irgendwie in meins einbetten (basierend auf iterPerEnv starten)



- Entscheiden, welche Grafiken ich alles brauche & dann das nötige dafür implementieren (Welche gruppierungen brauche ich?)
    - Gruppierungen für 2curric vs 3 vs 4 (vs 1 evtl auch noch) (auch für nGen und curricLength; damit ich den teil abschließen kann !)

- noch mehr neue Runs starten mit den neuen Rewardstruktur.
- (PPO besser verstehen, ggf manche sahcne zitieren aus dem paper oder aus einem anderen)

----------------------------
- all Curric plot useful? Kann man evtl besser zeigen ok: best curric sieht die sitr so aus, 1. step sieht sie so aus ...
- wenn jetzt multiobj, hätte ich nicht auch nsga 3 benutzen können / sollen ? --> Wie erkläre ich das
- RW: Einen RHEA CL Ansatz finden ??

------------------------------------------------------------------------------------------------------------


General todo:
+ ggf puren NSGA vs GA plot, wo nur experimente sind die in beiden varianten ausgeführt wurden
1 - (bei ssh aufräumen)
1 - evtl mit mehr als 1-2 CPUs ausprobieren
- RQ2 überarbeiten, weil ich ja gar nicht auf konvergenz untersucht habe
- evtl mal ein plot mit RRH CI/SD vs RHEA CL CI/SD, damit man die unterschiede besesr sehen kann
- Plot für den effekt, den man eigentlich erzielen möchte (Basics Teil, ggf Methodology)
- neue 25k 4s 3 3 run starten (Bzw testen warum es so unlucky war ?)
- climbing speed signifikanztest ???
- Elitismus und NSGA
+++
- Warum ist die iterationSteps egal bei dem initialen climb? Sollte es nicht gut machbar sein
- anhang: tabelle der experimente
- anhang: canceled experiments. ggf noch 1-2 sätze dazu schreiben wie weit es kam und was für performance das hatte
- No Curric Baseline (einfach 12x12 ausprobieren) useful ? Ist kein aufwand
- evtl noch einen anderen single obj starten ( --> falls man aussage verstärken möchte. Aber wird bei der geringen popzahl eh nix ausmachen)
- Rewardveränderungen der einzelnen envs plotten (ggf als 4er plot
- Performance plotten durch env distribution --> Neue Runs, damit man besser nachvollziehen kann, wie gut er in welcher env ist &
- aus dem meeting: whiteboard step-color plot (unter der performance!)
- RRH FIXEN weil ich ja den reward log angepasst habe (gamma struktur müsste wieder reingehholt werden glaube ich)
- Capitalize curriculum -> Curriculum


Prog:
    - Boxplots der gruppierungen zeigen (k steps, nGen, ...)

schreib:
1 - chapter referenzieren (in 1.3 und generell)
3 - vor und nachteile der einzelenn ansätze erläutern (basics)
3 - domain kapitel aufteilen in kleinere bzw einfach am rand erwähnen
3 - mehr zu CL und lokale minimum
3 - EA Quellen
3 - EA: roulette wheel ; fitness proportional usw verbessern
- \textit beim EA Kapitel: Wo genau hin überprüfen
- NSGA besser erklären, ggf mit einer Grafik & warum es eigentlich für multiobjetive gedacht ist) (auf corwding distance eingehen) & erwähnen, dass man es auch für single objectives benutzen kann
- RH Quelle
- RH: at first glance absatz verbessern --> diesen RH algorithmus vs iteration eines Algorithmus (ggf als begriff einführen)
- MCTS Quelle / RHEA Quelle
- vornachteile in basics kapitel einzelner ansätze
- domain basic teil rausnehmen (bzw in verkürzer form an anderen sachen einbringen)
- cl local minimum in more depth
- EA Quelle
- RW so anpassen, dass man je nach domain noch bestimmt gos/nogos erwähnt für die Algorithmen
- Sorgen: Begründung mit NSGA2. Warum überhaupt unterschiedlichen Effekt wenn single objective. Sehr reingeprägt als Multiobjective Problem
---> starker elitismus (auch in basic erläutern)
- multiobjective & singleobjective quelle da einbinde

Iwo einfügen
- diesen dip versuchen auszubalancieren, der durch die maxSteps entstanden ist wurde mal probiert
- Could have also made a stop at like .5 for 1kk and then go down all the way to .1 or so. (An alternative you could have used was to make it more like a step function, which stays the same for a bit of time, and the ncontinues to go down (right now it stays the same for 500k, then goes down linearly until .15, and then stays the same again
- exp setup: Anfang refrenz für die komplett-tabelle am ende





Low prio todo
- vllt mal die todos im code bearbeiten
- Docstrings alle vervollständigen
- README erweitern
- commits aufräumen


Schreib-Ideen
- Constraints einfügen für bessere balance (zb Relationship zwischen den einzelnen env-rewards --> Multi obj wäre besser) ; oder threshold rewards (ggf punishen, falls darunter)
- bayesian optimization -> Hyperparameter tuning hätte man damit vllt machen sollen statt so willkürlich



ka wo:
@Alex:
- ausprobierne mit mehr trainings envs als eval envs
- experiment mit mehr als 4 envs (zB empty & minigrid)
- CROSSOVER MUTATION PROBLEM
- 2 oder 3 RQs ? Weil stability kann bei performacne irgendwie schon direkt abgefrühstückt werden
----------------



Später
- Distributions mit SPLCL
- schauen ob gamma useful war: Berechne wie groß die summe gewesen wäre im ausgewählten Curriculum ohne Gamma für alle Curricula und schaue, ob die entscheidung dieselbe gewesen wäre

--------------------------------------------------------------------------------------------------------
Random:
- das ist super unclean geworden wegen der mutation/crossover probability und dass ich das jetzt hier und mal da geändert habe
- alle C runs beschränken auf durchschnitt oder min der frames done

Do:
d - RRH Traning time plot (wo die y achse auch passend ist und verlgiechbar mit den anderen)
d - time plots neu (alle zusammen, spezifisches NSGA vs GA)
d - time plot sortieren nach iterationSteps
d - Realtiver env distribution plot. Ggf auch env distr ohne SD anzeigen
d - SPLCL logs sind broken (aber ab wann`???)
d - ich glaub nen neues experiment machen, so dass ich sagen kann "zu hoch" ist schlecht fürCoMu
d - 1kk performacne: SPLCL fehlt
d - Experimente Tabelle machen (erstmal in google)
d + plot sortieren !! (250k muss rechts sein)
d - plot of snapshotScore vs actualScore


Fr
d eval --> args.procs mit args.episodes ersetzen (sofern es kleiner ist)
(halb)- das mit den distributions hinbekommen (1-2h)
(notizen)- Feedback einarbeiten (2-3h) (erstmal notizen machen)
d- anderes auch begradigen damit es einheitlich ist (zB manches RRH)
d? - SAC weitermachen
d - größere Achsenbeschriftungen
d - NSGA mehrdimensional (also 4 objectives



Sa
Schreibtag
step plot fix
paar kleinigkeiten

So
Ruhetag ?



Mo
viel überarbeiten im results kapitel; manche plots verbessert
d- SPLCL 10 runs starten;  (--> VORHER PULLEN)
d - NSGA multiobj run auf cluster starten
d - CL Baseline (REIHENFOLGE statt alles gleichzeitig !!!)
- SAC @home probiert (ging nicht starten)

Di:
- Distribution teil fertigmachen



----------------------------------------------------------------------------------
- internal / external validation ?
Stackedbarplot performance (NSGA vs GA vs RRH vs AP vs SPLCL)

Experimente:
- mit anderem ALgorithmus ( SAC / TD3 )
- mit / ohne Rewardshaping vergleichen
- mit anderen curricLengths (--stepsPerCurric)

Experiment Ideen:
- episoden pro eval verringern ( ziel schneller evaluieren)
- andere envGrößen
- andere Viewsize
- Andere Minigrid Envs?
- Ganz andere Envs?




- ggf neue experimente queuen, die den stepSize etwas rumprobieren (oderz umidnst schuaen wasi chs chonp robiert hatte und warum ich das nicht weiterverfolgt hatte usw)
- evtl mehr expeirmente mit norewardshasping machen
-- alte models automatisch löschen (ic hglaube das könnte schon ziemlich viel zeit sparen, damit git schneller ist und vllta uf dem cluster weniger schiefläuft. Aber ich weiß nicht genau, ob das blöd ist, wenn im Cluster immer so viel hin und hergeschrieben wird)


-------------------- FRAGEN ZUR MA -------------------
Fragen zur MA
- Wann erkläre ich denn das 1. mal unseren Ansatz? In den Basics bei RHEA? Weil in RW (Kapitel 3) möchte ich ja existierende sahen etwas mit unserem ansatz vergleiche
- Von welchem Wissensstand gehe ich für die Basics aus / was ist die Zielgruppe?
- Was für "cherry on top gibt es denn noch"?
- [1][2], oder [1,2]
- Std dev oder 95 CI ?
- appraoch vs algorithm

---------------------------------
- automatische model-benennung statt model param
- alte modelnamen löschen (also die obvious debug waren bzw abgebrochen, damit ich merh üebrsicht im ls habe)


- Ggf nicht immer 1. Schritt übernehmen des RHEA (ggf dynamisch machen), dynamische iterationLenghts1
- Ggf penalize, dass man immer gleiche envs nimmt ? Iwi encouragen dass er auch neue / sschwierige sachen ausprobiert (--> ggfh ängt das auch mit RH length zusammen)

Irgendwann Recherche:
- Wo performt hard-to-easy besser als CL und warum? Was ist die Idee dahinter überhaupt?


- RH Probleme? Was wenn man zB erst 2 Mio mal 16x16 trainieren muss, bevor man Fortschritte sieht? Wenn 1 Horizon bei 500k ist & die Iterationen pro Env noch niedriger sind
- minimize ersetzen auf das .hasNext() und dann abbrechen bei konvergenz (falls man später viele Gens machen will) --> could save time with EA

===========================================================================================

Changelog / Erkenntnisse
- Versucht 8x8 zu selten,16x16 sowieso
- schlechte Rewardstruktur --> - anpassen, wenn gewisse performance erreicht wird (-> ermutigen von 16x16)
- diff1 & 2 haben kaum unterschiede auf laufzeit
- maxSteps beeifnlusst performance (ggf berücksichtigen? Difficulty = 2 sollte ja nicht die performance schmälern, nur weil es schwerer ist)
-> ggf 2* max_reward - actual_reward * difficulty
- das dauert alles sehr lange und ist dann irgendwie nicht aussagefähig. --> ggf lieber 2-3 experimente für 16h laufen lassen statt 6x8h



- 8x8, vor allem 16x16 wurde fast nie ausgewählt. Daher Reward anpassen, um ermutigen das zu tun
- Reward am ende zu viel gewichtung
- envDifficulty (aka maxSteps) lässt reward etwas verzerrt aussehen; daher multiplier
- Difficulty Cutoffs: .25 / .75



17.05 - 24.05:
5x5 viewsize, dafür kleinere Level 4 7 9 12 (bzw 6 8 10 12) --> schicht entfernt im netz
-- Fix mit dem Tensorboard Bug
- bisschen history aufgeschrieben was wann warum passiert ist
- rewards normalisieren (damit man nicht verzerrt wird, ob es 3 oder 4 envs sind)
- Cuda broken ??
- Experiment: 4 Para trainieren Baseline  ( --> Reward instant bei 32 / 35); training mit pur 16x16 scheitert immer noch, paar h laufen lassen
- RRH Experiment (wollte eig mit neuen Envs testen ...) ; sehr jumpy ( --> colab)


25.05 - 30.05
?


31.05 - 9.6
- Difficulty verursacht zu große Spikes
- Difficulty zeitbasiert machen
- Cluster angefangen bzw. etwas startprobleme
- viel an der Evaluate rumgebastelt (so dass man jetzt mehreere models vergleichen kann)
- viele Experimente laufen lassen
- Baseline mit logs gemacht

10.06 - 20.06
- eval auf Dataframe umstellen statt händisch
- Clusterexperimente anfangen
- letzten bugs fixen
- anfangen schreiben


21.06 - 28.06
exerimente laufen lassen
schreiben


-------------===============================-----------------------=============---------------------------------------------------===
ZUR MA
60-80 Seiten
Pareto optimum (die linie für evol)
Sobel Sampling
am ende ggf bayesian optimization ( zB 2 Params gleichzeitg variieren lasesn)





============================================================
SEEDLISTE
 1
 1214
 2330
8515
 9152

 185
 1517
 3053
 2529
 8258

-------------------
 9030
 4221
 2450
 7477
 1938
 1291
 8646
 1032
 6646
 1315
 6471
 3259
 9607