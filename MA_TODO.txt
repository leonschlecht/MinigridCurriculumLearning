TEMPORARAY FIXED: BUG testen, ob trained 0 auch noch vorkommt, außer bei ep1gen1 (immer j =0)
done? Eval Zeit besser loggen als F: 9600 und sowas
already implemented @actualPeformacne Log: Used curriculum
done: print startEpoch / allepoch
done: starttime fix


Docstrings schreiben
Experimente-Evaluate ausprobieren, bzw gegeben eine json etwas rumprobieren / stubs erstellen, wie ich damit abeiten kann




"""
5x5 aufteilen , 6x6 rausnehmen ; 4x4 mit 2x2 obs

-  env schneller machen / laden
done: screen
- x register von performance abh.
- observation space reduzieren (komplexität des levels wird schwerer; und man kommt durch zufall leichter an) ; nutzlose iterationen sparen
    4, 6, 8, 10 (obs space macht es seeehr schwer)
"""


---------
Random Questions
- Kann man evtl partielle inforamtionen aus der letzten iteration mitnehmen ???
- Nehme ich das res.X, um dann einen RH zu simulieren oder nehme ich das beste Ergebnis der Simulationen?
- wie mit reward experimentieren dass man ihn ermutigt 16x16 auszuprobieren
- wie dynamisch mit dem switchen umgehen (zb nach jedem update mal probieren?)


----------------------------------------------------------------------------
TODO prov Env: die Rewardsscores für jedes Curriculum speichern (statt nur aufsummieren) -> Rewardscore des Mainmodels daraus ableiten (Jetzt hat es nur die Summe des RHs)
TODO ggf alte Models löschen, wenn man X epochs weiter ist. Solange man die relevanten logs noch hat

- Ist Speicherplatz eigentlich irgendwann ein Problem? die alten Models aus den Files könnte man ggf löschen, müsste dann aber noch mal sicherere gehen mit dem abspeichern
- welche performance metriken sollen überhaupt verwendet werden? Welche möchte man plotten später? (training vs eval reward, ...)

- Was für Env-Iterationen, Curriculum-Länge & -Anzahl werden überhaupt angestrebt?
- Kann man vllt so die Minigrid anpassen, dass ich nicht manuell auf register(...) in meinen inits zurückgreifen muss?
- Das Unlearning Problem: wie genau funktionieren die updates überhaupt (2560 update schritte, oder 1x pro level)
- Wie damit umgehen, dass man evtl nicht alles 1:1 gleich lang machen muss? zB Man kann 200k trainieren 16x16, und 100k 8x8 reicht zum stabilisieren
- RH Probleme? Was wenn man zB erst 2 Mio mal 16x16 trainieren muss, bevor man Fortschritte sieht? Wenn 1 Horizon bei 500k ist & die Iterationen pro Env noch niedriger sind
------------------------------------------

Paar Ideen
- Evaluation: use weights, maybe depending on progress ;  maybe use something else like Value / Policy Loss / Frames
# TODO maybe set biased pop for 1st epoch 1st generation like so: pop = Population.new("X", self.createFirstGeneration(self.curricula))


Small Refactorings
- use os . join for these things os.getcwd() + "\\storage\\" + directory)
- load time or set initially in linear / adaptive
- überall datetime einsetzen statt time

