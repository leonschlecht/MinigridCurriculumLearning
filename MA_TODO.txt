Todos:
- baseline + logs (ein mal alles para --> jeweils alle 250k eval
    ; dann 1x mit entscheiden pro epoch was als nächstes dran kommt)

- schauen, ob man das Register überhaupt braucht oder direkt in kwargs beim agenten das machen kann
- mit dem difficulty score etwas rumprobieren vllt ?
- alle relevanten Experimente backupen / bzw. das status & log davon



- 2. Minigrid Umgebung suchen (oder ertsmal Experimente weiter?);
- Cluster Test: läuft ohne fehler oder ggf OS Probleme?
- penv rumprobieren


Home:
- wie verändert sich die Eval-Zeit wenn man die Episoden erhöht zB auf 25? [vllt auch nur bei j=0 mehr episoden benutzen]
    - EVAL penv verbessern ; vllt kann man da ja was wiederverwenden; man kann evtl auch args.procs auf args.episdoes setzen ?

- schauen warum %-Performance falsch angezeigt wrude in den logs

==================================

Zeitplan
Mi: eval weiter; kann jetzt mehrere Sachen gleicheztiig anschauen
Do: tnt, dann eval weiter; überblick verschaffen; os.sep
Fr
Sa: wenig
So: wahrscheinlich nur exp laufen
Mo: -
Di: -
Mi
Do


===================================
Experimente:
2x2 Tage mit --iterPerStep 100000 --numCurric 3 --nGen 3 --stepsPerCurric 3 ( & 5)
1 Tag baseline (allPara)
1 Tag andere baseline
1 Tag RRH 4 curric
- auch mal mit anderem GA arbeiten --> Literatur


Allgemeines / Demnächst:
- visualisierungen: die Varianten-plots, um zu sehen was sich wie entwickelt hätte
- Anfangen zu schreiben (ggf 4h erstmal anfangen / Woche bis ende Juni)



- cuda noch mal probieren ??( https://www.run.ai/guides/gpu-deep-learning/pytorch-gpu )


- herausfinden, waru mdie logs vorher korrekt waren ??????? --> Fixen, dass man allF nicht einfach so hat, sondern eingelesen werdne muss
- check differences cmd line args & when reloading


Q's / Ideen:
- wie backupen die models (damit nicht alles futsch geht)
- Evaluationen frühzeitig cutten, wenn garbage?
- Das Unlearning Problem: wie genau funktionieren die updates überhaupt (2560 update schritte, oder 1x pro level)
- RH Probleme? Was wenn man zB erst 2 Mio mal 16x16 trainieren muss, bevor man Fortschritte sieht? Wenn 1 Horizon bei 500k ist & die Iterationen pro Env noch niedriger sind
------------------------------------------

- minimize ersetzen auf das .hasNext() und dann abbrechen bei konvergenz (falls man später viele Gens machen will)


Results-Evaluation
- eval: zählen welche env wie oft im training vorkam
- Experimente-Evaluate ausprobieren, bzw gegeben eine json etwas rumprobieren / stubs erstellen, wie ich damit abeiten kann



TODO ggf alte Models löschen, wenn man X epochs weiter ist. Solange man die relevanten logs noch hat
=========================================================================
MA Notizen

Vektor -> Matrix benennen (bzw. ggf 3d für die Repräsentation damit es übersichtlicher ist)

===========================================================================================

Changelog / Erkenntnisse
- Versucht 8x8 zu selten,16x16 sowieso
- schlechte Rewardstruktur --> - anpassen, wenn gewisse performance erreicht wird (-> ermutigen von 16x16)
- diff1 & 2 haben kaum unterschiede auf laufzeit
- maxSteps beeifnlusst performance (ggf berücksichtigen? Difficulty = 2 sollte ja nicht die performance schmälern, nur weil es schwerer ist)
-> ggf 2* max_reward - actual_reward * difficulty
- das dauert alles sehr lange und ist dann irgendwie nicht aussagefähig. --> ggf lieber 2-3 experimente für 16h laufen lassen statt 6x8h



- 8x8, vor allem 16x16 wurde fast nie ausgewählt. Daher Reward anpassen, um ermutigen das zu tun
- Reward am ende zu viel gewichtung
- envDifficulty (aka maxSteps) lässt reward etwas verzerrt aussehen; daher multiplier
- Difficulty Cutoffs: .25 / .75



17.05 - 24.05:
5x5 viewsize, dafür kleinere Level 4 7 9 12 (bzw 6 8 10 12) --> schicht entfernt im netz
-- Fix mit dem Tensorboard Bug
- bisschen history aufgeschrieben was wann warum passiert ist
- rewards normalisieren (damit man nicht verzerrt wird, ob es 3 oder 4 envs sind)
- Cuda broken ??
- Experiment: 4 Para trainieren Baseline  ( --> Reward instant bei 32 / 35); training mit pur 16x16 scheitert immer noch, paar h laufen lassen
- RRH Experiment (wollte eig mit neuen Envs testen ...) ; sehr jumpy ( --> colab)


25.05 - 30.05
?


31.05 - 9.6





-------------
===============================
Meetingnotes:
plotly  / seaborn
spikes sollten durch mehrfach ausführen geringer werden
AOC bestimmen
zB produzierte kurve (3) ; gleiche für (4), ... ---> in 1 grafik; auch vergleichen mit random, baseline usw;
- Dann Anzahl an Generationen
- Anzahl Level die bereitstellen (6 8 10 12 vs 5 6 8 16 zB)
einschärnkungen auch benenne ( alle kombinationen nicht möglich zu testen) -> es geht um Gefühl kriegen
am ende ggf bayesian optimization ( zB 2 Params gleichzeitg variieren lasesn)

-----------------------