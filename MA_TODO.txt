eval "$(~/tmp/Anaconda/home/schlecht/AnacondaNeu/bin/conda shell.bash hook)"



- Kapitel 1
- Mail
- Evaluation: 4x1 plots nebeneinander (oder sonst auch 2x2 geht antürlich auch)
-
war das schlimm mit den maxSteps ??? Dadurch dass man überall die 12x12 genommen hat.
und auch prüfen, ob das für andere experimente auch genommen wurde

- andere Env experimente queuen

- Neue Rewardlog experimente auswerten
- den selected Env plot irgendwie machen / überdenken


??
- @Minigrid: Training a PPO Agent ??? --> probier das mal aus i guess
-


---------------------------------------------

Next Week:
- Kapitel 4 & 5 abgeschickt (Netzwerk grafik?)
- Kapitel 1 & 2 & 3 fertigmachen
- SAC weiterprobieren oder halt neuen Algo
-


So:
- SAC zu hause rumprobieren (evtl auch einfach am laptop)
- Architekturgrafik vom model




----------------

- experimente ohne MAXSTEPS anpassung machen
-


Future work
- more objectives
- andere offspring raten
- varying von den parametern

-----

- Horizon definieren im basics teil als Wort ( = 1 Iteration vom RH)
- Limitation: hyperparameter sind sehr domänenabhängig
    PPO mit guten hyperparametern kann es das nicht doch lösen?
- ParaEnv im methodology: falls ich es da nicht hinpacke; ich müsste jedenfalls so oder so dazuschreiben dass das vllt nur für taskdriven cl sinn ergibt


wie kann ich das besser plotten ???
Vor allem wenn ich zb 5 experimente habe ?
- TODO für den plot
    - y achse wieder sortieren (achtung dass die values auch mitgemappt werden)
    - farben fixen
    - größe der punkte
    - überlegen,wie man das gut plotten kann
    - mach die y achse näher beinander
    - ggf etws anderes als kreise zum darstellen benutzen


Do:
- methodology & exp setup viel überarbeiten


Fr:
- Methodology & Exp setup weitermachen
- etwas den used env plot gearbeitet



Sa:
d - Pseudocode Methodology einbauen
- Methodology & Exp setup fertig
- SAC rumprobieren (3h)
- WENN ZEIT ÜBRIG: etwas am discussion chapter arbeiten

PLOT todo :
- einen nsga&ga filter einbauen
- Überall titel hinzufügen mit einheitlicher schriftgröße
- Legend immer hinzufügen + fontsize anpassen
- unnötigen krams aus legend rauscutten (also zb "Column" oder das n bei "6x6n")
- Größere x & y achsenbeschriftungen
- Bessere Gruppierungsmöglichkeiten einbauen, ggf auch mal subplots verwenden.
- Die x-tick labels einheitlich machen (zB Wan nsoll _NSGA dazukommen,
- Manche Ordner besser benennen (zB _step anpassen an das was ich eigentlich benutze)

SAC todo:
    schauen wie ich das speichern kann
    irgendwie in meins einbetten (basierend auf iterPerEnv starten)



Next week wichtig:
- SAC hinbekommen ODER anderen Algo (Oder es sein lassen)
(d) - Env Distributions genug text geschrieben(-> verstehen und erklären was wann passiert im algo).
- Mergen von dem Results chapter fertig
- Kapitel 4 & 5 fertig (2-3 Tage => 12h würde ich sagen)
- Kapitel 5: erklären der baselines
- Entscheiden, welche Grafiken ich alles brauche & dann das nötige dafür implementieren (Welche gruppierungen brauche ich?)
    - Gruppierungen für 2curric vs 3 vs 4 (vs 1 evtl auch noch) (auch für nGen und curricLength; damit ich den teil abschließen kann !)
- Grafiken alle neu machen
- Pseudocode für Methodology
- evtl die Legend bei den plots umbenennen (damit es einheitlich ist mit denn namen die ich bnenutze in meiner arbeit)
++
- neue Runs starten mit den neuen Rewardstruktur. Auch den Code dazu schreiben.
- Feedback eingearbeitet ;
- (PPO besser verstehen, ggf manche sahcne zitieren aus dem paper oder aus einem anderen)
-

----------------------------
@Alex:
- $...$ ahh
- aus welcher gen kommt das beste curriculum - interessant?
- all Curric plot useful? Kann man evtl besser zeigen ok: best curric sieht die sitr so aus, 1. step sieht sie so aus ...
- SAC: wie am besten ausprobieren?
- wenn jetzt multiobj, hätte ich nicht auch nsga 3 benutzen können / sollen ? --> Wie erkläre ich das
- 10 Crossover Mtation runs -> irgendwie erwähnen "out of curiosity ??"
- 1 Trainingsalgo: Wenn PPO es alleine nicht schafft, aber dann mit gleichen parametern und unserem Ansatz ...?
- \\ oder auch mal was anderes verwenden (was ist eig aus andere env / was anderes als minigrid verwenden geworden?)??
- RW: Einen RHEA CL Ansatz finden ??

schwachstellen benennen: 1 env
- fett machen
- 5 & 6 mergen
- 13: selection / generate new population
- 2: init population
- 13 updaet poupulation by selecting best individuals
- GetBestCurriculum(...)


------------------------------------------------------------------------------------------------------------


General todo:
+ ggf puren NSGA vs GA plot, wo nur experimente sind die in beiden varianten ausgeführt wurden
1 - (bei ssh aufräumen)
1 - evtl mit mehr als 1-2 CPUs ausprobieren
- RQ2 überarbeiten, weil ich ja gar nicht auf konvergenz untersucht habe
- evtl mal ein plot mit RRH CI/SD vs RHEA CL CI/SD, damit man die unterschiede besesr sehen kann
- Plot für den effekt, den man eigentlich erzielen möchte (Basics Teil, ggf Methodology)
- neue 25k 4s 3 3 run starten (Bzw testen warum es so unlucky war ?)
- climbing speed signifikanztest ???
- Elitismus und NSGA
+++
- Warum ist die iterationSteps egal bei dem initialen climb? Sollte es nicht gut machbar sein
- anhang: tabelle der experimente
- anhang: canceled experiments. ggf noch 1-2 sätze dazu schreiben wie weit es kam und was für performance das hatte
- No Curric Baseline (einfach 12x12 ausprobieren) useful ? Ist kein aufwand
- evtl noch einen anderen single obj starten ( --> falls man aussage verstärken möchte. Aber wird bei der geringen popzahl eh nix ausmachen)
- Rewardveränderungen der einzelnen envs plotten (ggf als 4er plot
- Performance plotten durch env distribution --> Neue Runs, damit man besser nachvollziehen kann, wie gut er in welcher env ist &
- aus dem meeting: whiteboard step-color plot (unter der performance!)
- RRH FIXEN weil ich ja den reward log angepasst habe (gamma struktur müsste wieder reingehholt werden glaube ich)
- Capitalize curriculum -> Curriculum


Prog:
    - Boxplots der gruppierungen zeigen (k steps, nGen, ...)

schreib:
1 - chapter referenzieren (in 1.3 und generell)
3 - vor und nachteile der einzelenn ansätze erläutern (basics)
3 - domain kapitel aufteilen in kleinere bzw einfach am rand erwähnen
3 - mehr zu CL und lokale minimum
3 - EA Quellen
3 - EA: roulette wheel ; fitness proportional usw verbessern
- \textit beim EA Kapitel: Wo genau hin überprüfen
- NSGA besser erklären, ggf mit einer Grafik & warum es eigentlich für multiobjetive gedacht ist) (auf corwding distance eingehen) & erwähnen, dass man es auch für single objectives benutzen kann
- RH Quelle
- RH: at first glance absatz verbessern --> diesen RH algorithmus vs iteration eines Algorithmus (ggf als begriff einführen)
- MCTS Quelle / RHEA Quelle
- vornachteile in basics kapitel einzelner ansätze
- domain basic teil rausnehmen (bzw in verkürzer form an anderen sachen einbringen)
- cl local minimum in more depth
- EA Quelle
- RW so anpassen, dass man je nach domain noch bestimmt gos/nogos erwähnt für die Algorithmen
- Sorgen: Begründung mit NSGA2. Warum überhaupt unterschiedlichen Effekt wenn single objective. Sehr reingeprägt als Multiobjective Problem
---> starker elitismus (auch in basic erläutern)
- multiobjective & singleobjective quelle da einbinde

Iwo einfügen
- diesen dip versuchen auszubalancieren, der durch die maxSteps entstanden ist wurde mal probiert
- Could have also made a stop at like .5 for 1kk and then go down all the way to .1 or so. (An alternative you could have used was to make it more like a step function, which stays the same for a bit of time, and the ncontinues to go down (right now it stays the same for 500k, then goes down linearly until .15, and then stays the same again
- exp setup: Anfang refrenz für die komplett-tabelle am ende





Low prio todo
- vllt mal die todos im code bearbeiten
- Docstrings alle vervollständigen
- README erweitern
- commits aufräumen


Schreib-Ideen
- Constraints einfügen für bessere balance (zb Relationship zwischen den einzelnen env-rewards --> Multi obj wäre besser) ; oder threshold rewards (ggf punishen, falls darunter)
- bayesian optimization -> Hyperparameter tuning hätte man damit vllt machen sollen statt so willkürlich



ka wo:
@Alex:
- ausprobierne mit mehr trainings envs als eval envs
- experiment mit mehr als 4 envs (zB empty & minigrid)
- CROSSOVER MUTATION PROBLEM
- 2 oder 3 RQs ? Weil stability kann bei performacne irgendwie schon direkt abgefrühstückt werden
----------------



Später
- Distributions mit SPLCL
- schauen ob gamma useful war: Berechne wie groß die summe gewesen wäre im ausgewählten Curriculum ohne Gamma für alle Curricula und schaue, ob die entscheidung dieselbe gewesen wäre

--------------------------------------------------------------------------------------------------------
Random:
- das ist super unclean geworden wegen der mutation/crossover probability und dass ich das jetzt hier und mal da geändert habe
- alle C runs beschränken auf durchschnitt oder min der frames done

Do:
d - RRH Traning time plot (wo die y achse auch passend ist und verlgiechbar mit den anderen)
d - time plots neu (alle zusammen, spezifisches NSGA vs GA)
d - time plot sortieren nach iterationSteps
d - Realtiver env distribution plot. Ggf auch env distr ohne SD anzeigen
d - SPLCL logs sind broken (aber ab wann`???)
d - ich glaub nen neues experiment machen, so dass ich sagen kann "zu hoch" ist schlecht fürCoMu
d - 1kk performacne: SPLCL fehlt
d - Experimente Tabelle machen (erstmal in google)
d + plot sortieren !! (250k muss rechts sein)
d - plot of snapshotScore vs actualScore


Fr
d eval --> args.procs mit args.episodes ersetzen (sofern es kleiner ist)
(halb)- das mit den distributions hinbekommen (1-2h)
(notizen)- Feedback einarbeiten (2-3h) (erstmal notizen machen)
d- anderes auch begradigen damit es einheitlich ist (zB manches RRH)
d? - SAC weitermachen
d - größere Achsenbeschriftungen
d - NSGA mehrdimensional (also 4 objectives



Sa
Schreibtag
step plot fix
paar kleinigkeiten

So
Ruhetag ?



Mo
viel überarbeiten im results kapitel; manche plots verbessert
d- SPLCL 10 runs starten;  (--> VORHER PULLEN)
d - NSGA multiobj run auf cluster starten
d - CL Baseline (REIHENFOLGE statt alles gleichzeitig !!!)
- SAC @home probiert (ging nicht starten)

Di:
- Distribution teil fertigmachen



----------------------------------------------------------------------------------
- internal / external validation ?
Stackedbarplot performance (NSGA vs GA vs RRH vs AP vs SPLCL)

Experimente:
- mit anderem ALgorithmus ( SAC / TD3 )
- mit / ohne Rewardshaping vergleichen
- mit anderen curricLengths (--stepsPerCurric)

Experiment Ideen:
- episoden pro eval verringern ( ziel schneller evaluieren)
- andere envGrößen
- andere Viewsize
- Andere Minigrid Envs?
- Ganz andere Envs?




- ggf neue experimente queuen, die den stepSize etwas rumprobieren (oderz umidnst schuaen wasi chs chonp robiert hatte und warum ich das nicht weiterverfolgt hatte usw)
- evtl mehr expeirmente mit norewardshasping machen
-- alte models automatisch löschen (ic hglaube das könnte schon ziemlich viel zeit sparen, damit git schneller ist und vllta uf dem cluster weniger schiefläuft. Aber ich weiß nicht genau, ob das blöd ist, wenn im Cluster immer so viel hin und hergeschrieben wird)


Heute:
- Kapitel 4 V1 fertig
- überlegen, welche Grafiken in Kapitel 4 passen könnten
- mit dem SAC weiterprobieren

- Grafik mit SPLCL Runs (bzw allen)
- 6 8 10 12 switch baseline (welches CL ist das? Gibt es iwi eine Literatur, die das vorschlägt)

Morgen:
- überlegen, wie ich alle 1kk env distribution oder so hinbekomme ( AUFGABENSTELLUNG !!!)
- Kapitel 4 fertig;
- Kapitel 5 etwas verfeinert
- Überlegen, welche runs noch fehlen (und die dann starten)

Fr:
- Kapitel 5 fertig
- Kapitel 6 etwas verfeinert
- Runs alle rüberkopiert, crMu Run5050 nach -> 54 56

Sa: Kapitel 6 fertig

So:
Proofreading K4,5,6 & abschicken


Andere Todos:
- Experimente:
-   - Mehr Runs ohne Rewrad Shaping
    - mit anderem algorithmus
    - mit anderen Gamma raten (evtl kombinieren mit curric Längen)
     mit anderen curricLengths (--stepsPerCurric)
     - weniger episoden pro eval
     - andere EnvGrößen & viewsize
     - andere minigrid envs?
- SPLCL runs rüberkopieren
- Grafiken zu SPLCL runs erstellen
- Time investieren in Groupbarchart (brauch generell für eig alles ist das useful)
- Verbessern der benutzen grafiken für die MA (--> vorher überlegen, welche ich eigentlihc brauche bzw. welche ich generieren kann und was für aussagen ich treffen möchte)
- eval README(und ggf kleine doc schreiben dmait es consistent ist und ich nicht jedes mal in iwelche probelem laufe)
- Kapitel 4 schreiben
- Kapitel 5 schreiben
- Kapitel 6 schreiben


Todos:
- scatterplot evtl für training time (NSGA, ..)
- xtick 45 grad & ---> verschieben ha=right (- rotation=45, ha='right')
- filter nach Methode (zB 100k, oder alle 3Gen, ...) --> schauen was ich noch nicht alles implemetneirt habe und wonach ich noch filtern önnte
- Idee vom RHEA CL Plot machen (kannst ja eigene daten erstellen dafür)
    - auch ein mal actual progression visualisieren (wie sich etwas in 1 epoche entwickelt hat)
!! Baselines mal genauer aufschlüsseln und schaune was ich schon habe, bzw was ich noch leicht implementieren könnte
- filter nach Methode (zB 100k, oder alle 3Gen, ...)


- I could use difficult for the pool of available curricula as well (to better adjust to the models performacne); so it would have more to choose from, but then the evol optim would take a lot longer
-Plot: time of 2 curric vs 3 curric vs 4 curric
Plot: Env distribution of NSGA vs GA (vs random rh vs splcl)
(plot: promising experiments that were cut due to training time)

- env distributions anschuaen,
- env distribution normalisieren
- 2 Grafiken einbinden (objetive function smoothing Nr1, und vllt die data driven / model driven #2)
stackedBarplot: heatmap: zeile curricula, spalte environments; Rot markeiren was oft ist und weiß was weniger




---------------------------
Low Prio
- (Multiobjective für NSGA machen)


Ideen
-------------------- FRAGEN ZUR MA -------------------
General Q:
- anderer termin ok
- etwas burnt out
- plots auf falscher seite (Tipps?)
- Benutzen von "we"?
- Wann erkläre ich denn das 1. mal unseren Ansatz? In den Basics bei RHEA? Weil in RW (Kapitel 3) möchte ich ja existierende sahen etwas mit unserem ansatz vergleiche
- SAC / ...  (wie am besten einbetten; erst mit minigrid gangbar machen -> dann ?)
- Was für "cherry on top gibt es denn noch"?
- [1][2], oder [1,2]
- Wie umgeht man dass der plot nicht im falschen kapitel landet ? (und auch balance zwischen Plotgröße und Lesbarkeit)


================

- Pool der available curricula verändern (schrieben aufgefallen)

-------------------- FRAGEN ZUR MA -------------------
Fragen zur MA
- Schreibstil, wie sehr den leser involvieren? "Now we are looking at ...", "As you can imagine ..."
- Wann erkläre ich denn das 1. mal unseren Ansatz? In den Basics bei RHEA? Weil in RW (Kapitel 3) möchte ich ja existierende sahen etwas mit unserem ansatz vergleiche
- Von welchem Wissensstand gehe ich für die Basics aus / was ist die Zielgruppe?
- Was für "cherry on top gibt es denn noch"?
- [1][2], oder [1,2]
- Std dev oder 95 CI ?
- appraoch vs algorithm
- Von welchem Wissensstand gehe ich für die Basics aus / was ist die Zielgruppe?




---------------------------------

Finde heraus wie weit alle experimente gekommen sind. --> Dann entscheide ggf mit 4kk

- automatische model-benennung statt model param
- alte modelnamen löschen (also die obvious debug waren bzw abgebrochen, damit ich merh üebrsicht im ls habe)




Random Notes / Ideas:
- optimize training times: early stopping / depending on threshold; or use a timelimit --> then combine w other methods (but probably bad since bad information to the EA)
- Ggf nicht immer 1. Schritt übernehmen des RHEA (ggf dynamisch machen), dynamische iterationLenghts1
- Ggf penalize, dass man immer gleiche envs nimmt ? Iwi encouragen dass er auch neue / sschwierige sachen ausprobiert (--> ggfh ängt das auch mit RH length zusammen)
- Ein automatischer Difficulty measurer wäre gut. Vllt kombinieren mit einem pre-trained model (dafür gabs iwo ein Paper)
- I could use difficult for the pool of available curricula as well (to better adjust to the models performacne); so it would have more to choose from, but then the evol optim would take a lot longer
- Überlegen wie man eine TSCL baseline machen könnte (bzw warum ich das nicht tue)


Irgendwann Recherche:
- Wo performt hard-to-easy besser als CL und warum? Was ist die Idee dahinter überhaupt?




- Ausprobieren mit 8 Envs oder so (zB mit den Empties)
- Ggf den Eval so umändern, dass man sich auf aktuellen Skill fokussiert (also zB nur 8x8 für alle "ähnlichen" vs immer alles durch; gerade bei > 4 envs wird es auch schwer.
    ; ggf auch iwi past in betracht ziehen
    Man könnte wahrscheinlich auch besser paralelisieren (wenn args.procs > args.episodes ist)




=============================================================

# TODO plot the snapshot vs curricReward problem ---> do some experiments. How does the curricLength influence results? How does gamma influence results?
Random Gedanken
- Das Unlearning Problem: wie genau funktionieren die updates überhaupt (2560 update schritte, oder 1x pro level)
- RH Probleme? Was wenn man zB erst 2 Mio mal 16x16 trainieren muss, bevor man Fortschritte sieht? Wenn 1 Horizon bei 500k ist & die Iterationen pro Env noch niedriger sind
- minimize ersetzen auf das .hasNext() und dann abbrechen bei konvergenz (falls man später viele Gens machen will) --> could save time with EA

===========================================================================================

Changelog / Erkenntnisse
- Versucht 8x8 zu selten,16x16 sowieso
- schlechte Rewardstruktur --> - anpassen, wenn gewisse performance erreicht wird (-> ermutigen von 16x16)
- diff1 & 2 haben kaum unterschiede auf laufzeit
- maxSteps beeifnlusst performance (ggf berücksichtigen? Difficulty = 2 sollte ja nicht die performance schmälern, nur weil es schwerer ist)
-> ggf 2* max_reward - actual_reward * difficulty
- das dauert alles sehr lange und ist dann irgendwie nicht aussagefähig. --> ggf lieber 2-3 experimente für 16h laufen lassen statt 6x8h



- 8x8, vor allem 16x16 wurde fast nie ausgewählt. Daher Reward anpassen, um ermutigen das zu tun
- Reward am ende zu viel gewichtung
- envDifficulty (aka maxSteps) lässt reward etwas verzerrt aussehen; daher multiplier
- Difficulty Cutoffs: .25 / .75



17.05 - 24.05:
5x5 viewsize, dafür kleinere Level 4 7 9 12 (bzw 6 8 10 12) --> schicht entfernt im netz
-- Fix mit dem Tensorboard Bug
- bisschen history aufgeschrieben was wann warum passiert ist
- rewards normalisieren (damit man nicht verzerrt wird, ob es 3 oder 4 envs sind)
- Cuda broken ??
- Experiment: 4 Para trainieren Baseline  ( --> Reward instant bei 32 / 35); training mit pur 16x16 scheitert immer noch, paar h laufen lassen
- RRH Experiment (wollte eig mit neuen Envs testen ...) ; sehr jumpy ( --> colab)


25.05 - 30.05
?


31.05 - 9.6
- Difficulty verursacht zu große Spikes
- Difficulty zeitbasiert machen
- Cluster angefangen bzw. etwas startprobleme
- viel an der Evaluate rumgebastelt (so dass man jetzt mehreere models vergleichen kann)
- viele Experimente laufen lassen
- Baseline mit logs gemacht

10.06 - 20.06
- eval auf Dataframe umstellen statt händisch
- Clusterexperimente anfangen
- letzten bugs fixen
- anfangen schreiben


21.06 - 28.06
exerimente laufen lassen
schreiben


-------------===============================
Meetingnotes:
einschärnkungen auch benenne ( alle kombinationen nicht möglich zu testen) -> es geht um Gefühl kriegen

-----------------------

ALTE NOTES
- ggf nur 1 leichte Env ist zu wenig
- 100k iterationen pro step zu wenig

=============---------------------------------------------------===
ZUR MA
60-80 Seiten
Pareto optimum (die linie für evol)
Sobel Sampling (funzt wahrsch nur für reel-wertige)
am ende ggf bayesian optimization ( zB 2 Params gleichzeitg variieren lasesn)





============================================================
SEEDLISTE
 1
 9152
 2330
 1214
 8515

 2529
 8258
 1517
  185
 3053

 9030
 4221
 2450
 7477
 1938
 1291
 8646
 1032
 6646
 1315
 6471
 3259
 9607