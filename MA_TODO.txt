eval "$(~/tmp/Anaconda/home/schlecht/AnacondaNeu/bin/conda shell.bash hook)"

General Qs
- Trainigszeiten limitieren sinnvoll? (zB 96h)



Fragen zur MA
- Benutzen von "we"?
- Wann erkläre ich denn das 1. mal unseren Ansatz? In den Basics bei RHEA? Weil in RW (Kapitel 3) möchte ich ja existierende sahen etwas mit unserem ansatz vergleichen
- Experimente
    - Andere viewsize probieren? (kann ich ja ggf 1 variante nehmen und für beide machen (dann auch für GA UND NSGA)
    - Mit evol parametern rumspielen
    - Mit Problemdefinition rumspielen ---> zB auf Multiobjective gehen
    - Andere Minigrid Envs?
    - Anderer Trainingsalgorithmus (vllt so wie mit der viewsize als validierung einfach für 2-3 sachen *2 dann machen)
    - Ganz andere Envs?
- Wie kann man das erklären mit dem "ja durch 10x10 trainieren wurde er im 12x12 besser; aber immer beim 12x12 trainieren hat er alles verlernt"
- Von welchem Wissensstand gehe ich für die Basics aus / was ist die Zielgruppe?
- TSCL ist doch relativ similar zu unserem Ansatz?
- Was für "cherry on top gibt es denn noch"?


Random Notes / Ideas:
- optimize training times: early stopping / depending on threshold; or use a timelimit --> other methods
- Ggf nicht immer 1. Schritt übernehmen des RHEA (ggf dynamisch machen), dynamische iterationLenghts
- Ggf penalize, dass man immer gleiche envs nimmt ? Iwi encouragen dass er auch neue / sschwierige sachen ausprobiert (--> ggfh ängt das auch mit RH length zusammen)
- Ein automatischer Difficulty measurer wäre gut. Vllt kombinieren mit einem pre-trained model (dafür gabs iwo ein Paper)
-

Irgendwann Recherche:
- Wo performt hard-to-easy besser als CL und warum? Was ist die Idee dahinter überhaupt?



Tasks:
- 1.1 Motivation
- 1.2 RQs
- 1.3 Structure
- 2.1 CL
- 2.2 EvolAlg
- 2.3 RH & RHEA
----> - 3. RW
    - First Application shorten
- 4. Methodology
- 5. Experimenal Setup
- 6.1 Results
- 6.2 Discussion
- 6.3 Limitations
- 7. Outlook


- Curriculum Progressions visualisieren ;
- Ggf auch mal die verschiedenen Tensboards anschauen, um den Trainingsreward der Snapshots zu visualisieren
- Bessere Grafiken:
    x-Achse ist shit
    evtl eine --model filter option
    Option für mehrere Models gleichzeitig vergleichen (vllt kleine kommandozeilen anwendung; mit zahlen zum eingeben
- Baselines für SPL
- Daten für No-Baseline approach (also nur PPO ?)
- Ausprobieren mit 8 Envs oder so (zB mit den Empties)
- Ggf den Eval so umändern, dass man sich auf aktuellen Skill fokussiert (also zB nur 8x8 für alle "ähnlichen" vs immer alles durch; gerade bei > 4 envs wird es auch schwer.
    ; ggf auch iwi past in betracht ziehen
    Man könnte wahrscheinlich auch besser paralelisieren (wenn args.procs > args.episodes ist)
- max y eingeben für die experimente (dann aber auch iwi loggen dass mehr anzeigbar wäre)


Mi
- RW weiter
- Genug experimente Queuen
- Tabelle updaten


Do
- RW progress
- Methodology anfangen
-



Fr
- Intro fertig
- Basics fertig


Sa
- Auswertung


So:
- Auswertung zu ende machen
-


- 2 Grafiken einbinden (smoothing Nr1, und vllt die data driven / model driven #2)
- Ich brauche eine Self paced baseline & evtl ein teacher-student
- Baseline trainieren ohne CL (random order der samples) --> Quasi 1 epoch mit 4 batches
- think about how to plot the curriculum progressions
- a normalized env distr (and keep other one too)
stackedBarplot: heatmap: zeile curricula, spalte environments; Rot markeiren was oft ist und weiß was weniger



=============================================================

# TODO the plotDistributionOfAllCurric should not have a shared x-axis; or at least still use epochs and not scale ???
# TODO plot progression of curricula against one another
# TODO plot showing differences between earlier and later generations
# TODO plot the snapshot vs curricReward problem ---> do some experiments. How does the curricLength influence results? How does gamma influence results?

todos:
- 2. Minigrid Umgebung suchen (oder ertsmal Experimente weiter?);


===================================================================

Random Gedanken
- Das Unlearning Problem: wie genau funktionieren die updates überhaupt (2560 update schritte, oder 1x pro level)
- RH Probleme? Was wenn man zB erst 2 Mio mal 16x16 trainieren muss, bevor man Fortschritte sieht? Wenn 1 Horizon bei 500k ist & die Iterationen pro Env noch niedriger sind
- minimize ersetzen auf das .hasNext() und dann abbrechen bei konvergenz (falls man später viele Gens machen will)


=========================================================================
MA Notizen

Vektor -> Matrix benennen (bzw. ggf 3d für die Repräsentation damit es übersichtlicher ist)

===========================================================================================

Changelog / Erkenntnisse
- Versucht 8x8 zu selten,16x16 sowieso
- schlechte Rewardstruktur --> - anpassen, wenn gewisse performance erreicht wird (-> ermutigen von 16x16)
- diff1 & 2 haben kaum unterschiede auf laufzeit
- maxSteps beeifnlusst performance (ggf berücksichtigen? Difficulty = 2 sollte ja nicht die performance schmälern, nur weil es schwerer ist)
-> ggf 2* max_reward - actual_reward * difficulty
- das dauert alles sehr lange und ist dann irgendwie nicht aussagefähig. --> ggf lieber 2-3 experimente für 16h laufen lassen statt 6x8h



- 8x8, vor allem 16x16 wurde fast nie ausgewählt. Daher Reward anpassen, um ermutigen das zu tun
- Reward am ende zu viel gewichtung
- envDifficulty (aka maxSteps) lässt reward etwas verzerrt aussehen; daher multiplier
- Difficulty Cutoffs: .25 / .75



17.05 - 24.05:
5x5 viewsize, dafür kleinere Level 4 7 9 12 (bzw 6 8 10 12) --> schicht entfernt im netz
-- Fix mit dem Tensorboard Bug
- bisschen history aufgeschrieben was wann warum passiert ist
- rewards normalisieren (damit man nicht verzerrt wird, ob es 3 oder 4 envs sind)
- Cuda broken ??
- Experiment: 4 Para trainieren Baseline  ( --> Reward instant bei 32 / 35); training mit pur 16x16 scheitert immer noch, paar h laufen lassen
- RRH Experiment (wollte eig mit neuen Envs testen ...) ; sehr jumpy ( --> colab)


25.05 - 30.05
?


31.05 - 9.6
- Difficulty verursacht zu große Spikes
- Difficulty zeitbasiert machen
- Cluster angefangen bzw. etwas startprobleme
- viel an der Evaluate rumgebastelt (so dass man jetzt mehreere models vergleichen kann)
- viele Experimente laufen lassen
- Baseline mit logs gemacht

10.06 - 20.06
- eval auf DF umstellen
- Clusterexperimente anfangen
- letzten bugs fixen
- anfangen schreiben


21.06 - 28.06
?


-------------===============================
Meetingnotes:
einschärnkungen auch benenne ( alle kombinationen nicht möglich zu testen) -> es geht um Gefühl kriegen

-----------------------

ALTE NOTES
- ggf nur 1 leichte Env ist zu wenig
- 100k iterationen pro step zu wenig
- mit dem difficulty hochgehen macht immense Probleme. Quasi 1 wasted epoch dann; ggf langsamer hochgehen


=============---------------------------------------------------
Pareto optimum (die linie für evol)
Sobel Sampling (funzt wahrsch nur für reel-wertige)
am ende ggf bayesian optimization ( zB 2 Params gleichzeitg variieren lasesn)




===
ZUR MA
60-80 Seiten


============================================================
SEEDLISTE
 1
 9152
 2330
 1214
 8515
 2529
 8258
 1517
  185
 3053
 9030
 4221
 2450
 7477
 1938
 1291
 8646
 1032
 6646
 1315
 6471
 3259
 9607