eval "$(~/tmp/Anaconda/home/schlecht/AnacondaNeu/bin/conda shell.bash hook)"

python .\evaluateCurriculum.py --iterGroup --env door --xIterations 5000000
python .\evaluateCurriculum.py --gen --env door --xIterations 5000000
python .\evaluateCurriculum.py --curricCount  --env door --xIterations 5000000
python .\evaluateCurriculum.py --curricLen --env door --xIterations 5000000

python .\evaluateCurriculum.py --comparisons 2 --curricDistr --normalize --title 'NSGA-II Multi vs Single Objective Environment Distribution'
python scores: gamma, multi, rs --> TITLE




DO:
- gucken ob sich das noch lohnt mit dem curricScore für multi obj zu fixen im result (irgendwie ist bei curricRewards etwas nicht richtig überschreiben worden) [müsste eig nur aufsummieren jeweils in der curricScore spalte] bzw bin mir auch nicht sicher ob das nicht verbuggt ist und an manchen stellen dann nicht richtig übernommen wurde, also idk)
- Ordner gruppieren(dyn obstacle runs)
- Dyn obstacle auswertung (vor allem score, aber auch environment distribution

- Überlegen, ob ich die Auswertung deutlich verkürzen kann, weil sich macnhes doppelt (lohnen sich diese plots am anfang halt überhaupt ; oder sollte ich vllt doch erst lieber beim vergleich mit den baselines einzelne runs zeigen?)
-



DK
100 324 (damit es fairer als 323 ist)
RRH 100k 9c 3s
RRH 150k 6c 3s
-------

- PLOT: Wie nah ist das bestCurriculum am snapshot score? Evtl kombinieren mit den Gruppierten plots teilweise ? [geht nur mit neuen runs]: - Vllt links snaphsotscore & rechts curricScore. ?
- PLOT: methode, um y achse korrekt zu sortieren (string vs nummern) [@distributions]
- PLOT: top 5 nsga vs top 5 ga grouped

- als csv ein mal das komplette dataframe speichern
- Auswirkungen von seed auf performance ?? vllt gibts ja immer einen, wo alle verkakcen / easy vorankommen...
- env distribution dyn obstacle
- Baselines mit mind. 1 RHEA CL run zeigen (zumindest spcl und ap, weil es empty ist)
((Dynamic Obstacle Runs)) -> Mindestens eine Grafik mit allen runs)


- ich müsstem al mehr das narvekar survey anschauen i guess ???
- evtl mit dem /benchmark ordner ausprobieren, damit ich nicht alles im root habe
- schauen wie HEM / ACL funktioniert, weil das schon irgendwie komisch wäre bei doorkey (die anderen steps wären dann gar nicht notwendig gewesen)
- TSCL ausprobierne per code ? weil es ja kein teacher network gibt sondern die heuristik genutzt werden könnte
- [13] Devin Soni. Introduction to evolutionary algorithms ---> Die Url muss in der bibliography auftauchen??
- Warum haben wir Minigrid genommen? gibt nichts vergleichbares für uns; aber auch ewgen doorkey und so. Ist schon eshr spezifisch, müsste evtl begründet werden

-----------------------------------------------------------------------------------------------------------------------------------

VariablenNamen:
- iterationSteps
- curricLength ( _3cstep
- nGen ( _3gen)
- curricCount (oder doch lieber numCurric?) (_3curric)
- paraEnv
- curricScore


@alex:
- Training time: sonst vergleichen die leute ja immer nur epochs / iterations & nicht sowas wie RH dass man quasi wasted simulations hat ?
- Wie oft zitieren? Also manchmal ergibt es sich im absatz durch They usw ja von selbst ?
- Wie erkläre ich das mit TSCL? / Warum ich das nicht benutzt habe
- limitations & future work dopplungen ?
- Lieber 1. schritt des curriculums oder gesamten curriculum score zeigen (Was algo sieht vs was wirklich ist)




------------


- curriculum progression plot (die hoffnung / idee) -- evtl in experimental setup oder bei RH in basic ? [evtl händisch erstellen ? 3curricula; 3-5 x ticks; gestirchelte linien usw]
- Reward / Gen Plot bzw vllt bester reward pro generation zeigen ??
- env1 usw plot
- curric score vs snapshot score ---> muss ich für die neuen runs erstn och erstellen im eval
- gen1 distr, vs gen2 distr, vs gen3 distr

- Ne Grafik, die zeigt, dass Gamma relevant ist und was sonst passieren könnte (Alle Curric Gleichgewichtet -> Schlechter snapshot gefahr
- Sind die nRS > RS oder tritt das nicht immer auf???
- Docstirngs & README & benchmark stuff anfangen zu schreiben
- Wie EA Initialisieren, wenn man train != eval envs. man könnte evtl. mehr envs zum Training verwenden könnte, als man in der Evaluierung benutzt, um Kosten zu sparen (zB TrainingsEnv: 6,8,10,12,14,16 und evaluierung mit 6, 8, 12, 16. Wie würde sich der EA dadurch verändern? Für single obj macht es glaube ich keinen Unterschied, aber für Multi Obj?
- AP aufteilen als oberklasse und 4 unterklassen
- RHEA cl vs RRH variance plot


================================================================================================
feedback
- more objectives = more sophisticated = larger pop required --> Quelle. (Habe ich selber argumentiert)
- "du beginnst das kapitel damit, dass man keinen einzelnen state-of-the-art definieren kann, sondern es von der jeweiligen anwendung abhängt. kann man guidelines geben in welchen fall welcher der vorgestellten algorithmen gut funktioniert und in welchen fällen vielleicht alle noch nachlegen müssten?"
- Du musst keine fastest method finden. Wenn die Endperformance in eigentlich allen Varianten ähnlich ist, warum ist diese Info dann nicht hilfreich?
- curricCount impacT: aber bei GA gab es doch das ergebnis auch ? (warum sollte das ein "recht wahrscheinliches ergebnis" sein bei NSGA?

=================================0

Changelog / Erkenntnisse
- Versucht 8x8 zu selten,16x16 sowieso
- schlechte Rewardstruktur --> - anpassen, wenn gewisse performance erreicht wird (-> ermutigen von 16x16)
- diff1 & 2 haben kaum unterschiede auf laufzeit
- maxSteps beeifnlusst performance (ggf berücksichtigen? Difficulty = 2 sollte ja nicht die performance schmälern, nur weil es schwerer ist)
-> ggf 2* max_reward - actual_reward * difficulty
- das dauert alles sehr lange und ist dann irgendwie nicht aussagefähig. --> ggf lieber 2-3 experimente für 16h laufen lassen statt 6x8h



- 8x8, vor allem 16x16 wurde fast nie ausgewählt. Daher Reward anpassen, um ermutigen das zu tun
- Reward am ende zu viel gewichtung
- envDifficulty (aka maxSteps) lässt reward etwas verzerrt aussehen; daher multiplier
- Difficulty Cutoffs: .25 / .75



17.05 - 24.05:
5x5 viewsize, dafür kleinere Level 4 7 9 12 (bzw 6 8 10 12) --> schicht entfernt im netz
-- Fix mit dem Tensorboard Bug
- bisschen history aufgeschrieben was wann warum passiert ist
- rewards normalisieren (damit man nicht verzerrt wird, ob es 3 oder 4 envs sind)
- Cuda broken ??
- Experiment: 4 Para trainieren Baseline  ( --> Reward instant bei 32 / 35); training mit pur 16x16 scheitert immer noch, paar h laufen lassen
- RRH Experiment (wollte eig mit neuen Envs testen ...) ; sehr jumpy ( --> colab)


25.05 - 30.05
?


31.05 - 9.6
- Difficulty verursacht zu große Spikes
- Difficulty zeitbasiert machen
- Cluster angefangen bzw. etwas startprobleme
- viel an der Evaluate rumgebastelt (so dass man jetzt mehreere models vergleichen kann)
- viele Experimente laufen lassen
- Baseline mit logs gemacht

10.06 - 20.06
- eval auf Dataframe umstellen statt händisch
- Clusterexperimente anfangen
- letzten bugs fixen
- anfangen schreiben


21.06 - 28.06
exerimente laufen lassen
schreiben


-------------===============================-----------------------=============---------------------------------------------------===
ZUR MA
60-80 Seiten
Pareto optimum (die linie für evol)
Sobel Sampling
am ende ggf bayesian optimization ( zB 2 Params gleichzeitg variieren lasesn)





============================================================
SEEDLISTE
 1
 1214
 2330
8515
 9152

 185
 1517
 3053
 2529
 8258

-------------------
 9030
 4221
 2450
 7477
 1938
 1291
 8646
 1032
 6646
 1315
 6471
 3259
 9607