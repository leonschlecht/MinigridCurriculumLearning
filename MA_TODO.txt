eval "$(~/tmp/Anaconda/home/schlecht/AnacondaNeu/bin/conda shell.bash hook)"

pc tmp datei rüberschieben


Neue Fragen
- Feste begriffe einführen, wie ich zB die hyperparameter nenne. zB curricStep statt "for every step of a curriculum"



- ggf neue experimente queuen, die den stepSize etwas rumprobieren (oderz umidnst schuaen wasi chs chonp robiert hatte und warum ich das nicht weiterverfolgt hatte usw)
- evtl mehr expeirmente mit norewardshasping machen


Heute:
- Kapitel 4 V1 fertig
- überlegen, welche Grafiken in Kapitel 4 passen könnten
- mit dem SAC weiterprobieren

- Grafik mit SPLCL Runs (bzw allen)
- 6 8 10 12 switch baseline (welches CL ist das? Gibt es iwi eine Literatur, die das vorschlägt)

Morgen:
- überlegen, wie ich alle 1kk env distribution oder so hinbekomme ( AUFGABENSTELLUNG !!!)
- Kapitel 4 fertig;
- Kapitel 5 etwas verfeinert
- Überlegen, welche runs noch fehlen (und die dann starten)

Fr:
- Kapitel 5 fertig
- Kapitel 6 etwas verfeinert
- Runs alle rüberkopiert, crMu Run5050 nach -> 54 56

Sa: Kapitel 6 fertig

So:
Proofreading K4,5,6 & abschicken


Andere Todos:
- Experimente:
-   - Mehr Runs ohne Rewrad Shaping
    - mit anderem algorithmus
    - mit anderen Gamma raten (evtl kombinieren mit curric Längen)
     mit anderen curricLengths (--stepsPerCurric)
     - weniger episoden pro eval
     - andere EnvGrößen & viewsize
     - andere minigrid envs?
- SPLCL runs rüberkopieren
- Grafiken zu SPLCL runs erstellen
- Time investieren in Groupbarchart (brauch generell für eig alles ist das useful)
- Verbessern der benutzen grafiken für die MA (--> vorher überlegen, welche ich eigentlihc brauche bzw. welche ich generieren kann und was für aussagen ich treffen möchte)
- eval README(und ggf kleine doc schreiben dmait es consistent ist und ich nicht jedes mal in iwelche probelem laufe)
- Kapitel 4 schreiben
- Kapitel 5 schreiben
- Kapitel 6 schreiben
- scatterplot evtl für training time (NSGA, ..)
- xtick 45 grad & ---> verschieben ha=right (- rotation=45, ha='right')
- filter nach Methode (zB 100k, oder alle 3Gen, ...) --> schauen was ich noch nicht alles implemetneirt habe und wonach ich noch filtern önnte
- Idee vom RHEA CL Plot machen (kannst ja eigene daten erstellen dafür)
    - auch ein mal actual progression visualisieren (wie sich etwas in 1 epoche entwickelt hat)
!! Baselines mal genauer aufschlüsseln und schaune was ich schon habe, bzw was ich noch leicht implementieren könnte
-Plot: time of 2 curric vs 3 curric vs 4 curric
Plot: Env distribution of NSGA vs GA (vs random)
(Plot: comparing GA with NSGA best run: etwas redundant, weil es eig schon in dem allPara plot mit drin ist)
(plot: promising experiments that were cut due to training time)

- env distributions anschuaen,
- env distribution normalisieren
- 2 Grafiken einbinden (objetive function smoothing Nr1, und vllt die data driven / model driven #2)
- Baseline trainieren ohne CL (random order der samples) --> Quasi 1 epoch mit 4 batches ( --> frag alex dazu)
stackedBarplot: heatmap: zeile curricula, spalte environments; Rot markeiren was oft ist und weiß was weniger




Low Prio
- (Multiobjective für NSGA)


Ideen
- Pool der available curricula verändern (schrieben aufgefallen)

-------------------- FRAGEN ZUR MA -------------------
Fragen zur MA
- Schreibstil, wie sehr den leser involvieren? "Now we are looking at ...", "As you can imagine ..."
- Wann erkläre ich denn das 1. mal unseren Ansatz? In den Basics bei RHEA? Weil in RW (Kapitel 3) möchte ich ja existierende sahen etwas mit unserem ansatz vergleiche
- Von welchem Wissensstand gehe ich für die Basics aus / was ist die Zielgruppe?
- Was für "cherry on top gibt es denn noch"?
- [1][2], oder [1,2]
- Std dev oder 95 CI ?
- appraoch vs algorithm


---------------------------------

- automatische model-benennung statt model param
- alte modelnamen löschen (also die obvious debug waren bzw abgebrochen, damit ich merh üebrsicht im ls habe)




Random Notes / Ideas:
- optimize training times: early stopping / depending on threshold; or use a timelimit --> then combine w other methods (but probably bad since bad information to the EA)
- Ggf nicht immer 1. Schritt übernehmen des RHEA (ggf dynamisch machen), dynamische iterationLenghts1
- Ggf penalize, dass man immer gleiche envs nimmt ? Iwi encouragen dass er auch neue / sschwierige sachen ausprobiert (--> ggfh ängt das auch mit RH length zusammen)
- Ein automatischer Difficulty measurer wäre gut. Vllt kombinieren mit einem pre-trained model (dafür gabs iwo ein Paper)
- I could use difficult for the pool of available curricula as well (to better adjust to the models performacne); so it would have more to choose from, but then the evol optim would take a lot longer
- Überlegen wie man eine TSCL baseline machen könnte (bzw warum ich das nicht tue)


Irgendwann Recherche:
- Wo performt hard-to-easy besser als CL und warum? Was ist die Idee dahinter überhaupt?




- Ausprobieren mit 8 Envs oder so (zB mit den Empties)
- Ggf den Eval so umändern, dass man sich auf aktuellen Skill fokussiert (also zB nur 8x8 für alle "ähnlichen" vs immer alles durch; gerade bei > 4 envs wird es auch schwer.
    ; ggf auch iwi past in betracht ziehen
    Man könnte wahrscheinlich auch besser paralelisieren (wenn args.procs > args.episodes ist)




=============================================================

# TODO plot the snapshot vs curricReward problem ---> do some experiments. How does the curricLength influence results? How does gamma influence results?
Random Gedanken
- Das Unlearning Problem: wie genau funktionieren die updates überhaupt (2560 update schritte, oder 1x pro level)
- RH Probleme? Was wenn man zB erst 2 Mio mal 16x16 trainieren muss, bevor man Fortschritte sieht? Wenn 1 Horizon bei 500k ist & die Iterationen pro Env noch niedriger sind
- minimize ersetzen auf das .hasNext() und dann abbrechen bei konvergenz (falls man später viele Gens machen will) --> could save time with EA

===========================================================================================

Changelog / Erkenntnisse
- Versucht 8x8 zu selten,16x16 sowieso
- schlechte Rewardstruktur --> - anpassen, wenn gewisse performance erreicht wird (-> ermutigen von 16x16)
- diff1 & 2 haben kaum unterschiede auf laufzeit
- maxSteps beeifnlusst performance (ggf berücksichtigen? Difficulty = 2 sollte ja nicht die performance schmälern, nur weil es schwerer ist)
-> ggf 2* max_reward - actual_reward * difficulty
- das dauert alles sehr lange und ist dann irgendwie nicht aussagefähig. --> ggf lieber 2-3 experimente für 16h laufen lassen statt 6x8h



- 8x8, vor allem 16x16 wurde fast nie ausgewählt. Daher Reward anpassen, um ermutigen das zu tun
- Reward am ende zu viel gewichtung
- envDifficulty (aka maxSteps) lässt reward etwas verzerrt aussehen; daher multiplier
- Difficulty Cutoffs: .25 / .75



17.05 - 24.05:
5x5 viewsize, dafür kleinere Level 4 7 9 12 (bzw 6 8 10 12) --> schicht entfernt im netz
-- Fix mit dem Tensorboard Bug
- bisschen history aufgeschrieben was wann warum passiert ist
- rewards normalisieren (damit man nicht verzerrt wird, ob es 3 oder 4 envs sind)
- Cuda broken ??
- Experiment: 4 Para trainieren Baseline  ( --> Reward instant bei 32 / 35); training mit pur 16x16 scheitert immer noch, paar h laufen lassen
- RRH Experiment (wollte eig mit neuen Envs testen ...) ; sehr jumpy ( --> colab)


25.05 - 30.05
?


31.05 - 9.6
- Difficulty verursacht zu große Spikes
- Difficulty zeitbasiert machen
- Cluster angefangen bzw. etwas startprobleme
- viel an der Evaluate rumgebastelt (so dass man jetzt mehreere models vergleichen kann)
- viele Experimente laufen lassen
- Baseline mit logs gemacht

10.06 - 20.06
- eval auf Dataframe umstellen statt händisch
- Clusterexperimente anfangen
- letzten bugs fixen
- anfangen schreiben


21.06 - 28.06
exerimente laufen lassen
schreiben


-------------===============================
Meetingnotes:
einschärnkungen auch benenne ( alle kombinationen nicht möglich zu testen) -> es geht um Gefühl kriegen

-----------------------

ALTE NOTES
- ggf nur 1 leichte Env ist zu wenig
- 100k iterationen pro step zu wenig

=============---------------------------------------------------===
ZUR MA
60-80 Seiten
Pareto optimum (die linie für evol)
Sobel Sampling (funzt wahrsch nur für reel-wertige)
am ende ggf bayesian optimization ( zB 2 Params gleichzeitg variieren lasesn)





============================================================
SEEDLISTE
 1
 9152
 2330
 1214
 8515

 2529
 8258
 1517
  185
 3053

 9030
 4221
 2450
 7477
 1938
 1291
 8646
 1032
 6646
 1315
 6471
 3259
 9607