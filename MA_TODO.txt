eval "$(~/tmp/Anaconda/home/schlecht/AnacondaNeu/bin/conda shell.bash hook)"

Morgen
- Teacherstudent Teil in RW fertig (also das 1 Paper durch & schon mit sätzen formuliert; 2. paper anfangen zu lesen und stichpunkte machen)
- Types of CL Teacherstudent hinzufügen & 1 andere methode



wie begründet man die EnvWahl? Welche Domain ist das genau?
Limitation: es gibt nur 4 envs; sonst könnte man evtl auch irgendwann sagen "hey xyz ist zu einfach, das müssen wir nicht ständig zeigen" ; die random init ist evtl etwas ineffizient für den Algorithmus, weil man die aktuellen fähigkeiten nicht berücksichtigt ;;
- L2: easy and hard might be more clear, if there were more environments / data to chooes from
- Unclear how RHEA with model-driven would work / look like or if it would even make sense in the slightest



TODO
- CL Types Kapitel (Fokussiere dich auf 5-6 Paper)
- 2 Grafiken einbinden (smoothing Nr1, und vllt die data driven / model driven #2)


--
- Ich brauche eine Self paced baseline & evtl ein teacher-student
- Baseline trainieren ohne CL (random order der samples) --> Quasi 1 epoch mit 4 batches

- think about how to plot the curriculum progressions




==============


GA RUNS
100k 3 3 3 -> 3.25kk noch
- sieht stabil aus, aber muss noch abwarten auf diff increase
100k 3 4gen 3 -> 3.5kk noch
- sehr stabil, aber noch auf diff increase warten
100k 4s 3 3 -> 3,5kk noch
- sehr jumpy?
150k 3 3 3
- sehr stabil
25k 3 3 3
- stabil aber noch viel zu wenig
50k 3 3 3
- sehr stabil, noch viel zu wenig
75k 3 3 3
- stabil, aber noch viel zu wenig
AllPara
- scheint schwach zu sein, aber konstant (evtl noch zu früh den absprung gehabt mit den difficulties)
AllPara const diff
- 1 Experiment erst; und nur 2kk iter
25k 7s 4g 3c
- sehr schneller anstieg, aber erst 400k und 1x


=================================================
- Why are some of the seed=1 runs having differnet snapshotscore length compared with the other runs ?
- should eval distr. also be normalized? Because it looks distorted otherwise
- Warum geht das mit dem snapshotscore bei manchen nicht( Bzw 1 unterschied? 75k 3 3 3 s1; oder 100k 4s 3g 3c_s1 ---> ggf wenn DONE ?

envDistribution aufteilen in einzelne spalten
stackedBarplot: heatmap: zeile curricula, spalte environments; Rot markeiren was oft ist und weiß was weniger




Themen:
- 4W 1. Draft
- Gliederung besprechen
- Bis nächstes mal: NSGA Experimente; Eval: was vorzeigen; Kapitel 1 & 2 schreiben
- models behalten oder löschen

NEXT W:
- anderen trainigsalgorithmus
- andere Envs?
- welche parameter alles testen?
- clusterjobs optimieren?


---------------------------------------------------


-normalize env distr (and keep other one too)





=============================================================

# TODO the plotDistributionOfAllCurric should not have a shared x-axis; or at least still use epochs and not scale ???



- Brauchen wir nicht einen anderen trainingsalgortihmus? Wie kann man das erklären mit dem "ja durch 10x10 trainieren wurde er im 12x12 besser; aber immer beim 12x12 trainieren hat er alles verlernt"



 # TODO plot progression of curricula against one another
# TODO plot showing differences between earlier and later generations
# TODO plot the snapshot vs curricReward problem ---> do some experiments. How does the curricLength influence results? How does gamma influence results?

todos:
- 2. Minigrid Umgebung suchen (oder ertsmal Experimente weiter?);


===================================================================

Random Gedanken
- Das Unlearning Problem: wie genau funktionieren die updates überhaupt (2560 update schritte, oder 1x pro level)
- RH Probleme? Was wenn man zB erst 2 Mio mal 16x16 trainieren muss, bevor man Fortschritte sieht? Wenn 1 Horizon bei 500k ist & die Iterationen pro Env noch niedriger sind
- minimize ersetzen auf das .hasNext() und dann abbrechen bei konvergenz (falls man später viele Gens machen will)


=========================================================================
MA Notizen

Vektor -> Matrix benennen (bzw. ggf 3d für die Repräsentation damit es übersichtlicher ist)

===========================================================================================

Changelog / Erkenntnisse
- Versucht 8x8 zu selten,16x16 sowieso
- schlechte Rewardstruktur --> - anpassen, wenn gewisse performance erreicht wird (-> ermutigen von 16x16)
- diff1 & 2 haben kaum unterschiede auf laufzeit
- maxSteps beeifnlusst performance (ggf berücksichtigen? Difficulty = 2 sollte ja nicht die performance schmälern, nur weil es schwerer ist)
-> ggf 2* max_reward - actual_reward * difficulty
- das dauert alles sehr lange und ist dann irgendwie nicht aussagefähig. --> ggf lieber 2-3 experimente für 16h laufen lassen statt 6x8h



- 8x8, vor allem 16x16 wurde fast nie ausgewählt. Daher Reward anpassen, um ermutigen das zu tun
- Reward am ende zu viel gewichtung
- envDifficulty (aka maxSteps) lässt reward etwas verzerrt aussehen; daher multiplier
- Difficulty Cutoffs: .25 / .75



17.05 - 24.05:
5x5 viewsize, dafür kleinere Level 4 7 9 12 (bzw 6 8 10 12) --> schicht entfernt im netz
-- Fix mit dem Tensorboard Bug
- bisschen history aufgeschrieben was wann warum passiert ist
- rewards normalisieren (damit man nicht verzerrt wird, ob es 3 oder 4 envs sind)
- Cuda broken ??
- Experiment: 4 Para trainieren Baseline  ( --> Reward instant bei 32 / 35); training mit pur 16x16 scheitert immer noch, paar h laufen lassen
- RRH Experiment (wollte eig mit neuen Envs testen ...) ; sehr jumpy ( --> colab)


25.05 - 30.05
?


31.05 - 9.6
- Difficulty verursacht zu große Spikes
- Difficulty zeitbasiert machen
- Cluster angefangen bzw. etwas startprobleme
- viel an der Evaluate rumgebastelt (so dass man jetzt mehreere models vergleichen kann)
- viele Experimente laufen lassen
- Baseline mit logs gemacht

10.06 - 20.06
- eval auf DF umstellen
- Clusterexperimente anfangen
- letzten bugs fixen
- anfangen schreiben


21.06 - 28.06
?


-------------===============================
Meetingnotes:
einschärnkungen auch benenne ( alle kombinationen nicht möglich zu testen) -> es geht um Gefühl kriegen

-----------------------

ALTE NOTES
- ggf nur 1 leichte Env ist zu wenig
- 100k iterationen pro step zu wenig
- mit dem difficulty hochgehen macht immense Probleme. Quasi 1 wasted epoch dann; ggf langsamer hochgehen


=============---------------------------------------------------
Pareto optimum (die linie für evol)
Sobel Sampling (funzt wahrsch nur für reel-wertige)
am ende ggf bayesian optimization ( zB 2 Params gleichzeitg variieren lasesn)




===
ZUR MA
60-80 Seiten


============================================================
SEEDLISTE
 1
 9152
 2330
 1214
 8515
 2529
 8258
 1517
  185
 3053
 9030
 4221
 2450
 7477
 1938
 1291
 8646
 1032
 6646
 1315
 6471
 3259
 9607