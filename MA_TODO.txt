eval "$(~/tmp/Anaconda/home/schlecht/AnacondaNeu/bin/conda shell.bash hook)"

Experimente:
- mit anderen Mutation / Crossover rates
- Reward auf 1 setzen
- mit anderem ALgorithmus ( SAC / TD3 )
-- alte models automatisch löschen (ic hglaube das könnte schon ziemlich viel zeit sparen, damit git schneller ist und vllta uf dem cluster weniger schiefläuft. Aber ich weiß nicht genau, ob das blöd ist, wenn im Cluster immer so viel hin und hergeschrieben wird)
md

Mi:
...


------
Themen
- Probleme mit Cluster
- Bisschen mehr auswertungsfilter gemacht
- Trainingsalgorithmus neu
- (Multiobjective)
- Pool der available curricula verändern (schrieben aufgefallen)
    [reward anpassen]
- ggf auf ersten 1kk mal fokussieren
# sobol sampling ( sklearn (0 bis 1) wären die mutations & crossover raten

rotation=45, ha='right'
https://medium.com/@antoine_savine/sobol-sequence-explained-188f422b246b
from scipy.stats import qmc
#set up sampler
sampler = qmc.Sobol(d=2, seed=42)
samples = sampler.random_base2(m=num_samples)


- wie schnell lernt es (1 grafik pro veränderter parameter)
- mehr curricula variationen
- scatterplot evtl für training time (NSGA, ..)
- Groupbarchart für alle 50k ; alle 100k .... )
- xtick 45 grad & ---> verschieben ha=right


Jetzt:
- Neue Experimente mit Crossover / Mutationsraten
- Mehr Text zu den Grafiken schreiben
- (SPLCL) bisschen rumprobieren wegen Parametern
- ((fix skip evaluation)

-------------------




- env distributions anschuaen,
- env distribution normalisieren



-- Runs checken (logs) (zusätzlich: splcl fixen )
- jetzt: filteredDistrDf weitermachen & splcl sicher anfangen
- filter nach Methode (zB 100k, oder alle 3Gen, ...)
- xtick 45deg


So:
- GA 50k runs starten für für 8515 & 9152
- SPLCL checken (aber scheint irrecoverable zu sein wenn er die schrittanzahl erhöht), sonst 5 Runs Queuen
- neuen Trainingsalgorithmus schauen versucen zu implementieren
- RRHs begradigen




make eval more efficient, so that not every DF is loaded on start
- 150k RRH auf 5kk cutten damit es cleaner aussieht
- Wenn man mit allPara vergleicht vllt auf 5kk beschränken, weil das sonst komisch aussieht



Plot: time of 2 curric vs 3 curric vs 4 curric
Plot: Env distribution of NSGA vs GA (vs random)
(Plot: comparing GA with NSGA best run: etwas redundant, weil es eig schon in dem allPara plot mit drin ist)
(plot: promising experiments that were cut due to training time)




General Q:
Wie umgeht man dass der plot nicht im falschen kapitel landet ?

#ERROR!
- I could use difficult for the pool of available curricula as well (to better adjust to the models performacne); so it would have more to choose from, but then the evol optim would take a lot longer
-




--------------

Finde heraus wie weit alle experimente gekommen sind.
Dann entscheide ggf mit 4kk

Heute:
- durch alle Ordner gehen und wenn fertig: alte model löschen
- google tabelle vollständig
- args param für welchen Reward man nehmen soll
- evaluateCurriculum laufen lassen / anschauen, ggf noch das distribution ding fertig machen
    auch so machen, dass man 1-4 runs leicht vergleichen kann.
- den prozess anpassen bzw schauen wegen slurmjob ??
- algo als param in der status json (evtl auch viewSize)
- automatische model-benennung statt model param
- alte modelnamen löschen (also die obvious debug waren bzw abgebrochen, damit ich merh üebrsicht im ls habe)

Neue Experimente
- episoden pro eval verringern ( ziel schneller evaluieren)
- Mit evol parametern rumspielen
- andere envGrößen
- andere viewsize (kann ich ja ggf 1 variante nehmen und für beide machen (dann auch für GA UND NSGA)
- Mit Problemdefinition rumspielen ---> zB auf Multiobjective gehen
- Andere Minigrid Envs?
- Anderer Trainingsalgorithmus (vllt so wie mit der viewsize als validierung einfach für 2-3 sachen *2 dann machen)
- Ganz andere Envs?



General Qs
- Trainigszeiten limitieren sinnvoll? (zB 96h)



Fragen zur MA
- Schreibstil, wie sehr den leser involvieren? "Now we are looking at ...", "As you can imagine ..."
- BEngl vs AEngl
- Benutzen von "we"?
- Wann erkläre ich denn das 1. mal unseren Ansatz? In den Basics bei RHEA? Weil in RW (Kapitel 3) möchte ich ja existierende sahen etwas mit unserem ansatz vergleiche

- Wie kann man das erklären mit dem "ja durch 10x10 trainieren wurde er im 12x12 besser; aber immer beim 12x12 trainieren hat er alles verlernt"
- Von welchem Wissensstand gehe ich für die Basics aus / was ist die Zielgruppe?
- TSCL ist doch relativ similar zu unserem Ansatz oder?
- Was für "cherry on top gibt es denn noch"?



Random Notes / Ideas:
- optimize training times: early stopping / depending on threshold; or use a timelimit --> other methods
- Ggf nicht immer 1. Schritt übernehmen des RHEA (ggf dynamisch machen), dynamische iterationLenghts
- Ggf penalize, dass man immer gleiche envs nimmt ? Iwi encouragen dass er auch neue / sschwierige sachen ausprobiert (--> ggfh ängt das auch mit RH length zusammen)
- Ein automatischer Difficulty measurer wäre gut. Vllt kombinieren mit einem pre-trained model (dafür gabs iwo ein Paper)
-

Irgendwann Recherche:
- Wo performt hard-to-easy besser als CL und warum? Was ist die Idee dahinter überhaupt?



Tasks:
- 1.1 Motivation
- 1.2 RQs
- 1.3 Structure
- 2.1 CL
- 2.2 EvolAlg
- 2.3 RH & RHEA
----> - 3. RW
    - First Application shorten
- 4. Methodology
- 5. Experimenal Setup
- 6.1 Results
- 6.2 Discussion
- 6.3 Limitations
- 7. Outlook






Sa
- 2h Intro
- RRH 50k hinbekommen (warum stop bei 50 iter? 250k macht es ja auch richtig..
++ RRH länger machen (zB 10kk)
- Andere Sachen schauen wie weit
- 2 Grafiken für Auswertung einbinden
    - ggf noch bei eval das "beste" MOdel je Algorithmus auswählen können
- Auswertung anfangen
- Ich brauche eine Self paced baseline ----> DONE
    & evtl ein teacher-student


+ vllt den Trainingsreward mal anschauen, um bestimmte Effekte besser nachvolziehen zu können

 Curriculum Progressions visualisieren ;
- Ggf auch mal die verschiedenen Tensboards anschauen, um den Trainingsreward der Snapshots zu visualisieren
- Bessere Grafiken:
    x-Achse ist shit
    evtl eine --model filter option
    Option für mehrere Models gleichzeitig vergleichen (vllt kleine kommandozeilen anwendung; mit zahlen zum eingeben
- Baselines für SPL
- Daten für No-Baseline approach (also nur PPO ?)
- Ausprobieren mit 8 Envs oder so (zB mit den Empties)
- Ggf den Eval so umändern, dass man sich auf aktuellen Skill fokussiert (also zB nur 8x8 für alle "ähnlichen" vs immer alles durch; gerade bei > 4 envs wird es auch schwer.
    ; ggf auch iwi past in betracht ziehen
    Man könnte wahrscheinlich auch besser paralelisieren (wenn args.procs > args.episodes ist)
- max y eingeben für die experimente (dann aber auch iwi loggen dass mehr anzeigbar wäre)



So:
- Auswertung zu ende machen
-


- 2 Grafiken einbinden (smoothing Nr1, und vllt die data driven / model driven #2)
- Baseline trainieren ohne CL (random order der samples) --> Quasi 1 epoch mit 4 batches
- think about how to plot the curriculum progressions
- a normalized env distr (and keep other one too)
stackedBarplot: heatmap: zeile curricula, spalte environments; Rot markeiren was oft ist und weiß was weniger





Mi
- RW weiter
- Genug experimente Queuen
- Tabelle updaten
Do
- RW progress
- Methodology anfangen
Fr
- Intro angefangen
- Basics EA und RH gemacht


=============================================================

# TODO the plotDistributionOfAllCurric should not have a shared x-axis; or at least still use epochs and not scale ???
# TODO plot progression of curricula against one another
# TODO plot showing differences between earlier and later generations
# TODO plot the snapshot vs curricReward problem ---> do some experiments. How does the curricLength influence results? How does gamma influence results?

todos:
- 2. Minigrid Umgebung suchen (oder ertsmal Experimente weiter?);


===================================================================

Random Gedanken
- Das Unlearning Problem: wie genau funktionieren die updates überhaupt (2560 update schritte, oder 1x pro level)
- RH Probleme? Was wenn man zB erst 2 Mio mal 16x16 trainieren muss, bevor man Fortschritte sieht? Wenn 1 Horizon bei 500k ist & die Iterationen pro Env noch niedriger sind
- minimize ersetzen auf das .hasNext() und dann abbrechen bei konvergenz (falls man später viele Gens machen will)


=========================================================================
MA Notizen

Vektor -> Matrix benennen (bzw. ggf 3d für die Repräsentation damit es übersichtlicher ist)

===========================================================================================

Changelog / Erkenntnisse
- Versucht 8x8 zu selten,16x16 sowieso
- schlechte Rewardstruktur --> - anpassen, wenn gewisse performance erreicht wird (-> ermutigen von 16x16)
- diff1 & 2 haben kaum unterschiede auf laufzeit
- maxSteps beeifnlusst performance (ggf berücksichtigen? Difficulty = 2 sollte ja nicht die performance schmälern, nur weil es schwerer ist)
-> ggf 2* max_reward - actual_reward * difficulty
- das dauert alles sehr lange und ist dann irgendwie nicht aussagefähig. --> ggf lieber 2-3 experimente für 16h laufen lassen statt 6x8h



- 8x8, vor allem 16x16 wurde fast nie ausgewählt. Daher Reward anpassen, um ermutigen das zu tun
- Reward am ende zu viel gewichtung
- envDifficulty (aka maxSteps) lässt reward etwas verzerrt aussehen; daher multiplier
- Difficulty Cutoffs: .25 / .75



17.05 - 24.05:
5x5 viewsize, dafür kleinere Level 4 7 9 12 (bzw 6 8 10 12) --> schicht entfernt im netz
-- Fix mit dem Tensorboard Bug
- bisschen history aufgeschrieben was wann warum passiert ist
- rewards normalisieren (damit man nicht verzerrt wird, ob es 3 oder 4 envs sind)
- Cuda broken ??
- Experiment: 4 Para trainieren Baseline  ( --> Reward instant bei 32 / 35); training mit pur 16x16 scheitert immer noch, paar h laufen lassen
- RRH Experiment (wollte eig mit neuen Envs testen ...) ; sehr jumpy ( --> colab)


25.05 - 30.05
?


31.05 - 9.6
- Difficulty verursacht zu große Spikes
- Difficulty zeitbasiert machen
- Cluster angefangen bzw. etwas startprobleme
- viel an der Evaluate rumgebastelt (so dass man jetzt mehreere models vergleichen kann)
- viele Experimente laufen lassen
- Baseline mit logs gemacht

10.06 - 20.06
- eval auf Dataframe umstellen statt händisch
- Clusterexperimente anfangen
- letzten bugs fixen
- anfangen schreiben


21.06 - 28.06
exerimente laufen lassen
schreiben


-------------===============================
Meetingnotes:
einschärnkungen auch benenne ( alle kombinationen nicht möglich zu testen) -> es geht um Gefühl kriegen

-----------------------

ALTE NOTES
- ggf nur 1 leichte Env ist zu wenig
- 100k iterationen pro step zu wenig

=============---------------------------------------------------===
ZUR MA
60-80 Seiten
Pareto optimum (die linie für evol)
Sobel Sampling (funzt wahrsch nur für reel-wertige)
am ende ggf bayesian optimization ( zB 2 Params gleichzeitg variieren lasesn)





============================================================
SEEDLISTE
 1
 9152
 2330
 1214
 8515

 2529
 8258
 1517
  185
 3053

 9030
 4221
 2450
 7477
 1938
 1291
 8646
 1032
 6646
 1315
 6471
 3259
 9607