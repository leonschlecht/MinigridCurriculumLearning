seed an modelnamen anhängen, dass ich das nicht immer in der sbatch 2x ändern muss


- seaborn / DF ==>
- diff anpassen
- Andere Experimente mit NSGA


TODO
- Schreiben anfangen
- Eval weiter (ID spalte) & mit barplots testen
-normalize env distr (and keep other one too)

- iterationsPerEnv Column


============
NOTES:



==================
Aktuelle Runs:
100k 3 3 3 => 6 done

100k 4step  => 1 fast done nach 40h
 1
 9152
 2330
 1214
 8515
 2529

50k (3 3 3) ==> 5 done
 2529

75k (3 3 3) ==> 3 done
 1214
 8515

150k => 5 done

100k 3 4gen 3 =>
1 DONE
9152 started
2330
1214
8515
2529

25k => seed 1noch nicht done;
1
 9152 DONE
 2330
 1214
 8515
 2529

long 25k => ??



python3 -m scripts.trainCurriculum --procs 48 --numCurric 3 --stepsPerCurric 4 --nGen 3 --iterPerEnv 100000 --model test12x12_100k_4step_3gen_3curric


===================

c -2 ==> c -3 wahrscheinlich ändern oder noch mehr, damit es schneller geht

Commands:
python -m scripts.trainCurriculum --iterPerEnv 5000 --procs 4 --model mo12312
eval "$(~/tmp/Anaconda/home/schlecht/AnacondaNeu/bin/conda shell.bash hook)"


- Baseline (
	alle 4 gleichzeitig;
	alle 4 dynamisch je nach performance;
	linear curriculum
- Experimente

- Vergleich mit SOTA (iwelche teacher-student krams vllt ??
- Visualisierungen
	pandas aggr
	einzelne Experimente filtern (zB Baseline oder Gen oder ...)
	Todos durchgehen



- Brauchen wir nicht einen anderen trainingsalgortihmus? Wie kann man das erklären mit dem "ja durch 10x10 trainieren wurde er im 12x12 besser; aber immer beim 12x12 trainieren hat er alles verlernt"



=================================================


Q's
Wie viele Experimente?
- Generationen ausprobieren (3,4,5. Noch mehr?
- iterationen pro Curric Schritt ausprobieren (50-200k)
- Curriclänge (3-5)
- anderen Evol Alg ausprobieren



-------------------------------------------
Für MA:
Difficulty Spikes: 1 Model zeigen & 1 linie anschauen (difficulty daneben plotten) ; spikes erklären dadurch

pandas benutzen: seaborn
https://seaborn.pydata.org/examples/errorband_lineplots.html

wie lang hat man auf welchen trainiert?


========================

Next:
- Cluster rumprobieren (
- 2. env oder 2. algo ausprobieren?
- evaluate merge fertigkriegen
- diverging curricula vergleich

- RRH run; dynamische baseline mit paraEnv; ...
- minimal mit schreiben anfangen


lineplot startpunkte
-----------
neue todos:

# mit paraEnv & RRH hinbekommen anzuzeigen bzw zu vergleichen (ggf Type in Result oder so als variable für easy access) (2h ~)
# 2. minigrid env suchen & wenigstens ausführbar machen (2h limit)
# Recherche für verwendete EA (2h limit)


 # TODO plot progression of curricula against one another
- überlegen, wie man sich am besten die Sachen vergleichen kann. Ggf parameter, oder liste an models
# TODO plot showing differences between earlier and later generations
# TODO plot the snapshot vs curricReward problem ---> do some experiments. How does the curricLength influence results? How does gamma influence results?


===============

Performance Plot:
-250k steps (oder 500k)
- überlegen wie man cutten kann bei ungelcihheit
- legend out of plot; und irgendwie zu viele Lines


Distribution plot:
- shared y axis? (oder vllt relative Anzahlen benutzen?)
- auch mit dem YMax umgehen
- Benennung -> 12x12 in title und dann aus der Legende raus (ggf einfach .split auf den Ordnernamen)
-

todos:
- baseline + logs (ein mal alles para --> jeweils alle 250k eval
    ; dann 1x mit entscheiden pro epoch was als nächstes dran kommt)
- 2. Minigrid Umgebung suchen (oder ertsmal Experimente weiter?);
- Cluster Test: läuft ohne fehler oder ggf OS Probleme?
- penv rumprobieren


- alle relevanten Experimente backupen / bzw. das status & log davon


Maybes:
- mit dem difficulty score etwas rumprobieren vllt ?



BESPRECHEN:
- difficulty score rausnehmen, bzw. nur max 1 Schritt machen (damit hyperparameter weniger und weniger verzerr ist)
- 2. Umgebung
- ssh Sachen (wichtig für Note?). + blockiert sein?
- bald anderen GA ausprobieren
- Nxt week: ?
- algo laden dauert .15s auf cluster




Unwichtig:
- schauen, ob man das Register überhaupt braucht oder direkt in kwargs beim agenten das machen kann



Home:
- wie verändert sich die Eval-Zeit wenn man die Episoden erhöht zB auf 25? [vllt auch nur bei j=0 mehr episoden benutzen]
    - EVAL penv verbessern ; vllt kann man da ja was wiederverwenden; man kann evtl auch args.procs auf args.episdoes setzen ?

- schauen warum %-Performance falsch angezeigt wrude in den logs

===================================================================
Experimente:
2x2 Tage mit --iterPerStep 100000 --numCurric 3 --nGen 3 --stepsPerCurric 3 ( & 5)
1 Tag baseline (allPara)
1 Tag andere baseline
1 Tag RRH 4 curric
- auch mal mit anderem GA arbeiten --> Literatur


Allgemeines / Demnächst:
- visualisierungen: die Varianten-plots, um zu sehen was sich wie entwickelt hätte
- Anfangen zu schreiben (ggf 4h erstmal anfangen / Woche bis ende Juni)



- cuda noch mal probieren ??( https://www.run.ai/guides/gpu-deep-learning/pytorch-gpu )


- herausfinden, waru mdie logs vorher korrekt waren ??????? --> Fixen, dass man allF nicht einfach so hat, sondern eingelesen werdne muss
- check differences cmd line args & when reloading


Q's / Ideen:
- wie backupen die models (damit nicht alles futsch geht)
- Evaluationen frühzeitig cutten, wenn garbage?
- Das Unlearning Problem: wie genau funktionieren die updates überhaupt (2560 update schritte, oder 1x pro level)
- RH Probleme? Was wenn man zB erst 2 Mio mal 16x16 trainieren muss, bevor man Fortschritte sieht? Wenn 1 Horizon bei 500k ist & die Iterationen pro Env noch niedriger sind
------------------------------------------

- minimize ersetzen auf das .hasNext() und dann abbrechen bei konvergenz (falls man später viele Gens machen will)


Results-Evaluation
- eval: zählen welche env wie oft im training vorkam
- Experimente-Evaluate ausprobieren, bzw gegeben eine json etwas rumprobieren / stubs erstellen, wie ich damit abeiten kann



TODO ggf alte Models löschen, wenn man X epochs weiter ist. Solange man die relevanten logs noch hat
=========================================================================
MA Notizen

Vektor -> Matrix benennen (bzw. ggf 3d für die Repräsentation damit es übersichtlicher ist)

===========================================================================================

Changelog / Erkenntnisse
- Versucht 8x8 zu selten,16x16 sowieso
- schlechte Rewardstruktur --> - anpassen, wenn gewisse performance erreicht wird (-> ermutigen von 16x16)
- diff1 & 2 haben kaum unterschiede auf laufzeit
- maxSteps beeifnlusst performance (ggf berücksichtigen? Difficulty = 2 sollte ja nicht die performance schmälern, nur weil es schwerer ist)
-> ggf 2* max_reward - actual_reward * difficulty
- das dauert alles sehr lange und ist dann irgendwie nicht aussagefähig. --> ggf lieber 2-3 experimente für 16h laufen lassen statt 6x8h



- 8x8, vor allem 16x16 wurde fast nie ausgewählt. Daher Reward anpassen, um ermutigen das zu tun
- Reward am ende zu viel gewichtung
- envDifficulty (aka maxSteps) lässt reward etwas verzerrt aussehen; daher multiplier
- Difficulty Cutoffs: .25 / .75



17.05 - 24.05:
5x5 viewsize, dafür kleinere Level 4 7 9 12 (bzw 6 8 10 12) --> schicht entfernt im netz
-- Fix mit dem Tensorboard Bug
- bisschen history aufgeschrieben was wann warum passiert ist
- rewards normalisieren (damit man nicht verzerrt wird, ob es 3 oder 4 envs sind)
- Cuda broken ??
- Experiment: 4 Para trainieren Baseline  ( --> Reward instant bei 32 / 35); training mit pur 16x16 scheitert immer noch, paar h laufen lassen
- RRH Experiment (wollte eig mit neuen Envs testen ...) ; sehr jumpy ( --> colab)


25.05 - 30.05
?


31.05 - 9.6
- Difficulty verursacht zu große Spikes
- Difficulty zeitbasiert machen
- Cluster angefangen bzw. etwas startprobleme
- viel an der Evaluate rumgebastelt (so dass man jetzt mehreere models vergleichen kann)
- viele Experimente laufen lassen
- Baseline mit logs gemacht






-------------
===============================
Meetingnotes:
plotly  / seaborn
spikes sollten durch mehrfach ausführen geringer werden
AOC bestimmen
zB produzierte kurve (3) ; gleiche für (4), ... ---> in 1 grafik; auch vergleichen mit random, baseline usw;
- Dann Anzahl an Generationen
- Anzahl Level die benutzt wurden (6 8 10 12 vs 5 6 8 16 zB)
einschärnkungen auch benenne ( alle kombinationen nicht möglich zu testen) -> es geht um Gefühl kriegen
am ende ggf bayesian optimization ( zB 2 Params gleichzeitg variieren lasesn)

-----------------------