Wichtig:
- viewsize wrapper hinbekommen / herausfinden was mit dem stride schiefgeht

- den 1st step reward speichern statt den gesamten reward benutzen ( = "actualPerformance" --> maxReward beurteilen daran; aber trotzdem das andere loggen)
- Curriculum-Rewards normalisieren (sonst ist das in der json irreführend)


Ladezeit-Optimierungen
- minimize ersetzen auf das .hasNext() und dann abbrechen bei konvergenz
- penv weiterprobieren ( Prozess pausieren, damit nicht 4 * 32 aktiv sind sondern nur 1 * 32)
- eval penv verbessern ; vllt kann man da ja was wiederverwenden (zumindest das letzte)

.........

Log-Improvements
"notes": als default hinzufügen & "manualNotes":
- ggf die envDetails Struktur flippen (epoch->gen->curric zu epoch->curric->gen)
- wird numFrames in der json richtig gesetzt? --> ich glaube ja?
- seed in der status.json abspeichern bzw auch laden wenn man weitermacht
- add Start-date / maybe automatcially at end of modelname
- ggf actual performance anpassen (weil also ein epoch_i zwischenschalten); ggf auch redundanz damit man es besser erkennen kann (also ein mal actualPerformacne wie es ist, und ein mal nur den ausgewählten Reward oder so
epochsDone: 1 zu hoch in der stauts.json (--> TESTEN MIT RHEA)




Results-Evaluation
- eval: zählen welche env wie oft im training vorkam
- Experimente-Evaluate ausprobieren, bzw gegeben eine json etwas rumprobieren / stubs erstellen, wie ich damit abeiten kann


Other
- Docstrings schreiben
- consec chosen einbauen in update methode für RRH biased







Erkenntnise:
- Versucht 8x8 zu selten,16x16 sowieso
- schlechte Rewardstruktur --> - anpassen, wenn gewisse performance erreicht wird (-> ermutigen von 16x16)
- diff1 & 2 haben kaum unterschiede auf laufzeit
- maxSteps beeifnlusst performance (ggf berücksichtigen? Difficulty = 2 sollte ja nicht die performance schmälern, nur weil es schwerer ist)
-> ggf 2* max_reward - actual_reward * difficulty

-----------------

experimente machen: RRH vs RHEA; anderes als GA ausprobieren
viewsizewrapper
2. Env raussuchen
algo fix mit dem laden bzw besser testen


/storage backupen & gut benennen, ggf irgendwo immer noch extra notes reinschreiben
eval vllt beschleunigen und 0 zurückgeben, wenn er zu oft verkackt hat bzw immer maxSteps braucht; evtl spätere envs skippen
Rewards-list in json ist ein string ?

Random Questions
- Kann man evtl partielle inforamtionen aus der letzten iteration mitnehmen ??? zB für den evol Alg.
- Nehme ich das res.X, um dann einen RH zu simulieren oder nehme ich das beste Ergebnis der Simulationen?
- wie mit reward experimentieren dass man ihn ermutigt 16x16 auszuprobieren
- wie dynamisch mit dem switchen umgehen (zb nach jedem update mal probieren?)
- funzt der 16x16 richtig (oder kann was wegen int-rounding schief gehen ; bzw. warum wird es nie gewählt?


"""
5x5 aufteilen , 6x6 rausnehmen ; 4x4 mit 2x2 obs
- observation space reduzieren (komplexität des levels wird schwerer; und man kommt durch zufall leichter an) ; nutzlose iterationen sparen
    4, 6, 8, 10 (obs space macht es seeehr schwer)
"""


-------------------------------------------------------------------------------------
TODO prov Env: die Rewardsscores für jedes Curriculum speichern (statt nur aufsummieren) -> Rewardscore des Mainmodels daraus ableiten (Jetzt hat es nur die Summe des RHs)
TODO ggf alte Models löschen, wenn man X epochs weiter ist. Solange man die relevanten logs noch hat

- Ist Speicherplatz eigentlich irgendwann ein Problem? die alten Models aus den Files könnte man ggf löschen, müsste dann aber noch mal sicherere gehen mit dem abspeichern
- welche performance metriken sollen überhaupt verwendet werden? Welche möchte man plotten später? (training vs eval reward, ...)

- Was für Env-Iterationen, Curriculum-Länge & -Anzahl werden überhaupt angestrebt?
- Kann man vllt so die Minigrid anpassen, dass ich nicht manuell auf register(...) in meinen inits zurückgreifen muss?
- Das Unlearning Problem: wie genau funktionieren die updates überhaupt (2560 update schritte, oder 1x pro level)
- Wie damit umgehen, dass man evtl nicht alles 1:1 gleich lang machen muss? zB Man kann 200k trainieren 16x16, und 100k 8x8 reicht zum stabilisieren
- RH Probleme? Was wenn man zB erst 2 Mio mal 16x16 trainieren muss, bevor man Fortschritte sieht? Wenn 1 Horizon bei 500k ist & die Iterationen pro Env noch niedriger sind
------------------------------------------

Paar Ideen
- Evaluation: use weights, maybe depending on progress ;  maybe use something else like Value / Policy Loss / Frames
# TODO maybe set biased pop for 1st epoch 1st generation like so: pop = Population.new("X", self.createFirstGeneration(self.curricula))


Small Refactorings
- use os . join for these things os.getcwd() + "\\storage\\" + directory)
- load time or set initially in linear / adaptive
- überall datetime einsetzen statt time

