eval "$(~/tmp/Anaconda/home/schlecht/AnacondaNeu/bin/conda shell.bash hook)"



Experimente:
- mit anderen Mutation / Crossover rates DONE
- Reward auf 1 setzen (gleichzeitig DONE)
- mit anderem ALgorithmus ( SAC / TD3 )
- mit anderen Gamma raten


-- alte models automatisch löschen (ic hglaube das könnte schon ziemlich viel zeit sparen, damit git schneller ist und vllta uf dem cluster weniger schiefläuft. Aber ich weiß nicht genau, ob das blöd ist, wenn im Cluster immer so viel hin und hergeschrieben wird)
md


Q:
- [1][2], oder [1,2]


Mi:
- Etwas von den Verbesserungen eingearbetiet
- SAC RL versucht gangbar zu machen


Do
- Experimente rüberkopieren, fertige sachen löschen
- Neue Experimente mit anderen Crossover / Mutationsraten starten
- SAC RL weiterprobieren ( 3-4h)
- 1-2h pur fürs schreiben investieren


Fr:
- SPLCL runs rüberkopieren
- Grafiken zu SPLCL runs erstellen
- Time investieren in Groupbarchart
- Verbessern der benutzen grafiken für die MA (--> vorher überlegen, welche ich eigentlihc brauche bzw. welche ich generieren kann und was für aussagen ich treffen möchte)

updated Fr.
d - SAC weiterprobieren
d - 2x 50min schreiben --> ggf noch Grafiken aufräumen oder leichte eval anpassungen machen, so dass ich es einfacher habe bzw die richtigen benutzen kann.
d - Mail an alex (was kommt jetzt genau in welches kapitel und wie unterscheiden sich methodology & experimental setup voneinander ?)


Sa:
- ggf was von gestern noch gucken
- SAC weitermachen
- eval wieder ausführbar (und ggf kleine doc schreiben dmait es consistent ist und ich nicht jedes mal in iwelche probelem laufe)
- 5.1 rq1 schreiben,
- 5.1 rq3 schreiben


Todos:
- ggf auf ersten 1kk mal fokussieren (Analyse: wer peakt schneller?)
- scatterplot evtl für training time (NSGA, ..)
- Groupbarchart für alle 50k ; alle 100k .... )
- xtick 45 grad & ---> verschieben ha=right (- rotation=45, ha='right')
- filter nach Methode (zB 100k, oder alle 3Gen, ...)

- 150k RRH auf 5kk cutten damit es cleaner aussieht
- Wenn man mit allPara vergleicht vllt auf 5kk beschränken, weil das sonst komisch aussieht


- I could use difficult for the pool of available curricula as well (to better adjust to the models performacne); so it would have more to choose from, but then the evol optim would take a lot longer
-Plot: time of 2 curric vs 3 curric vs 4 curric
Plot: Env distribution of NSGA vs GA (vs random)
(Plot: comparing GA with NSGA best run: etwas redundant, weil es eig schon in dem allPara plot mit drin ist)
(plot: promising experiments that were cut due to training time)

- env distributions anschuaen,
- env distribution normalisieren


Neue Experimente
- episoden pro eval verringern ( ziel schneller evaluieren)
- andere envGrößen
- andere Viewsize
- Andere Minigrid Envs?
- Ganz andere Envs?






------
Low Prio
- (Multiobjective für NSGA)

Themen:
General Q:
Wie umgeht man dass der plot nicht im falschen kapitel landet ? (und auch balance zwischen Plotgröße und Lesbarkeit)
- Pool der available curricula verändern (schrieben aufgefallen)

Fragen zur MA
- Schreibstil, wie sehr den leser involvieren? "Now we are looking at ...", "As you can imagine ..."
- BEngl vs AEngl
- Benutzen von "we"?
- Wann erkläre ich denn das 1. mal unseren Ansatz? In den Basics bei RHEA? Weil in RW (Kapitel 3) möchte ich ja existierende sahen etwas mit unserem ansatz vergleiche

- Wie kann man das erklären mit dem "ja durch 10x10 trainieren wurde er im 12x12 besser; aber immer beim 12x12 trainieren hat er alles verlernt"
- Von welchem Wissensstand gehe ich für die Basics aus / was ist die Zielgruppe?
- TSCL ist doch relativ similar zu unserem Ansatz oder?
- Was für "cherry on top gibt es denn noch"?



---------------------------------

Finde heraus wie weit alle experimente gekommen sind. --> Dann entscheide ggf mit 4kk

- algo als param in der status json (evtl auch viewSize)
- automatische model-benennung statt model param
- alte modelnamen löschen (also die obvious debug waren bzw abgebrochen, damit ich merh üebrsicht im ls habe)




Random Notes / Ideas:
- optimize training times: early stopping / depending on threshold; or use a timelimit --> other methods
- Ggf nicht immer 1. Schritt übernehmen des RHEA (ggf dynamisch machen), dynamische iterationLenghts1
- Ggf penalize, dass man immer gleiche envs nimmt ? Iwi encouragen dass er auch neue / sschwierige sachen ausprobiert (--> ggfh ängt das auch mit RH length zusammen)
- Ein automatischer Difficulty measurer wäre gut. Vllt kombinieren mit einem pre-trained model (dafür gabs iwo ein Paper)

Irgendwann Recherche:
- Wo performt hard-to-easy besser als CL und warum? Was ist die Idee dahinter überhaupt?



- Überlegen wie man eine TSCL baseline machen könnte (bzw warum ich das nicht tue)
!! Baselines mal genauer aufschlüsseln und schaune was ich schon habe, bzw was ich noch leicht implementieren könnte
- Curriculum Progressions visualisieren --> zumindest mal, um die Idee klarer zu machen (Ggf auch mal die verschiedenen Tensboards anschauen, um den Trainingsreward der Snapshots zu visualisieren)

- Ausprobieren mit 8 Envs oder so (zB mit den Empties)
- Ggf den Eval so umändern, dass man sich auf aktuellen Skill fokussiert (also zB nur 8x8 für alle "ähnlichen" vs immer alles durch; gerade bei > 4 envs wird es auch schwer.
    ; ggf auch iwi past in betracht ziehen
    Man könnte wahrscheinlich auch besser paralelisieren (wenn args.procs > args.episodes ist)



- 2 Grafiken einbinden (objetive function smoothing Nr1, und vllt die data driven / model driven #2)
- Baseline trainieren ohne CL (random order der samples) --> Quasi 1 epoch mit 4 batches ( --> frag alex dazu)
stackedBarplot: heatmap: zeile curricula, spalte environments; Rot markeiren was oft ist und weiß was weniger





=============================================================

# TODO plot the snapshot vs curricReward problem ---> do some experiments. How does the curricLength influence results? How does gamma influence results?
- Gamma anpassung experimente (schauen was wie viel bringt usw)


Random Gedanken
- Das Unlearning Problem: wie genau funktionieren die updates überhaupt (2560 update schritte, oder 1x pro level)
- RH Probleme? Was wenn man zB erst 2 Mio mal 16x16 trainieren muss, bevor man Fortschritte sieht? Wenn 1 Horizon bei 500k ist & die Iterationen pro Env noch niedriger sind
- minimize ersetzen auf das .hasNext() und dann abbrechen bei konvergenz (falls man später viele Gens machen will) --> could save time with EA


=========================================================================
MA Notizen

Vektor -> Matrix benennen (bzw. ggf 3d für die Repräsentation damit es übersichtlicher ist)

===========================================================================================

Changelog / Erkenntnisse
- Versucht 8x8 zu selten,16x16 sowieso
- schlechte Rewardstruktur --> - anpassen, wenn gewisse performance erreicht wird (-> ermutigen von 16x16)
- diff1 & 2 haben kaum unterschiede auf laufzeit
- maxSteps beeifnlusst performance (ggf berücksichtigen? Difficulty = 2 sollte ja nicht die performance schmälern, nur weil es schwerer ist)
-> ggf 2* max_reward - actual_reward * difficulty
- das dauert alles sehr lange und ist dann irgendwie nicht aussagefähig. --> ggf lieber 2-3 experimente für 16h laufen lassen statt 6x8h



- 8x8, vor allem 16x16 wurde fast nie ausgewählt. Daher Reward anpassen, um ermutigen das zu tun
- Reward am ende zu viel gewichtung
- envDifficulty (aka maxSteps) lässt reward etwas verzerrt aussehen; daher multiplier
- Difficulty Cutoffs: .25 / .75



17.05 - 24.05:
5x5 viewsize, dafür kleinere Level 4 7 9 12 (bzw 6 8 10 12) --> schicht entfernt im netz
-- Fix mit dem Tensorboard Bug
- bisschen history aufgeschrieben was wann warum passiert ist
- rewards normalisieren (damit man nicht verzerrt wird, ob es 3 oder 4 envs sind)
- Cuda broken ??
- Experiment: 4 Para trainieren Baseline  ( --> Reward instant bei 32 / 35); training mit pur 16x16 scheitert immer noch, paar h laufen lassen
- RRH Experiment (wollte eig mit neuen Envs testen ...) ; sehr jumpy ( --> colab)


25.05 - 30.05
?


31.05 - 9.6
- Difficulty verursacht zu große Spikes
- Difficulty zeitbasiert machen
- Cluster angefangen bzw. etwas startprobleme
- viel an der Evaluate rumgebastelt (so dass man jetzt mehreere models vergleichen kann)
- viele Experimente laufen lassen
- Baseline mit logs gemacht

10.06 - 20.06
- eval auf Dataframe umstellen statt händisch
- Clusterexperimente anfangen
- letzten bugs fixen
- anfangen schreiben


21.06 - 28.06
exerimente laufen lassen
schreiben


-------------===============================
Meetingnotes:
einschärnkungen auch benenne ( alle kombinationen nicht möglich zu testen) -> es geht um Gefühl kriegen

-----------------------

ALTE NOTES
- ggf nur 1 leichte Env ist zu wenig
- 100k iterationen pro step zu wenig

=============---------------------------------------------------===
ZUR MA
60-80 Seiten
Pareto optimum (die linie für evol)
Sobel Sampling (funzt wahrsch nur für reel-wertige)
am ende ggf bayesian optimization ( zB 2 Params gleichzeitg variieren lasesn)





============================================================
SEEDLISTE
 1
 9152
 2330
 1214
 8515

 2529
 8258
 1517
  185
 3053

 9030
 4221
 2450
 7477
 1938
 1291
 8646
 1032
 6646
 1315
 6471
 3259
 9607