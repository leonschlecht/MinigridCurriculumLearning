print max reweard like 35 / ... 50
curricScore zeigt nicht den multiplied wert


Q / Ideen
- wie viele evaluationen laufen lassen. Einfach cutten, wenn garbage?

Do:
nimmt ganzes modell statt snapshot (zB speichert ep1_curric0_gen3 ---> ep2 statt ep1_curric0_gen3_canddate --> ep2)
- herausfinden, waru mdie logs vorher korrekt waren ???????
- check differences cmd line args in reloading


Aufgaben:
- Funktioniert Cuda richtig? https://www.run.ai/guides/gpu-deep-learning/pytorch-gpu
------ 6 8 10 12 für 5x5 viwesize (3x3 macht wenig Sinn)
------- baseline mit paraEnv trainieren
- Experimente-History aufschreiben + Gedanken / Resultate was wozu geführt hatte etc


===================================
L:
should curricEvnDetails be renamed to acoomodate the fact that it is only appended at end with [epoch]

Home:
- testen, ob das toggle-env einen verbuggeten Reward-Log hat. ( zB --> 5x5 * 2 ausprobieren; dann ein malmit 5x5 / 16x16)
- testen, ob pymoo in 1 Generation immer allse richtig weitergibt und nix ausversehen doppelt passiert (zB schauen ob andere Curricula verwendet werden)


------- storage in git pushen
------------ testen, ob RHEA auch richtig durchläuft mit den Generationen
--------- RHEA mit FESTEN envs testen (um zu sehen, ob er auch richtig weitermacht bei dem switch bzw sehen wo tensorboard scheitert)
- epochsDone: 1 zu hoch in der stauts.json (--> TESTEN MIT RHEA) (steht 12 drin; womit würde es jetzt weitermachen?) also das "START EPOCH ...."
- self.iterationsPerEnv reload testen

Q's:
den 1st step reward speichern statt den gesamten reward benutzen ( = "actualPerformance" --> maxReward beurteilen daran; aber trotzdem das andere loggen)
- Curriculum-Rewards normalisieren wichtig? (sonst ist das in der json irreführend)

===========================================================================


Ladezeit-Optimierungen
- minimize ersetzen auf das .hasNext() und dann abbrechen bei konvergenz
- penv weiterprobieren ( Prozess pausieren, damit nicht 4 * 32 aktiv sind sondern nur 1 * 32)
- eval penv verbessern ; vllt kann man da ja was wiederverwenden (zumindest das letzte)


Log-Improvements
- ggf die envDetails Struktur flippen (epoch->gen->curric zu epoch->curric->gen) 1/3

Results-Evaluation
- eval: zählen welche env wie oft im training vorkam
- Experimente-Evaluate ausprobieren, bzw gegeben eine json etwas rumprobieren / stubs erstellen, wie ich damit abeiten kann


Other
- Docstrings schreiben
- gamma Parameter auswirkungen; summe von 0 oder 1 starten lassen
 wird maxReward richtig geestzt / ist er noch relevant?


Erkenntnise:
- Versucht 8x8 zu selten,16x16 sowieso
- schlechte Rewardstruktur --> - anpassen, wenn gewisse performance erreicht wird (-> ermutigen von 16x16)
- diff1 & 2 haben kaum unterschiede auf laufzeit
- maxSteps beeifnlusst performance (ggf berücksichtigen? Difficulty = 2 sollte ja nicht die performance schmälern, nur weil es schwerer ist)
-> ggf 2* max_reward - actual_reward * difficulty
- das dauert alles sehr lange und ist dann irgendwie nicht aussagefähig. --> ggf lieber 2-3 experimente für 16h laufen lassen statt 6x8h

-----------------

experimente machen: RRH vs RHEA; anderes als GA ausprobieren
viewsizewrapper
2. Env raussuchen
algo fix mit dem laden bzw besser testen


/storage backupen & gut benennen, ggf irgendwo immer noch extra notes reinschreiben
eval vllt beschleunigen und 0 zurückgeben, wenn er zu oft verkackt hat bzw immer maxSteps braucht; evtl spätere envs skippen
Rewards-list in json ist ein string ?

Random Questions
- Kann man evtl partielle inforamtionen aus der letzten iteration mitnehmen ??? zB für den evol Alg.
- Nehme ich das res.X, um dann einen RH zu simulieren oder nehme ich das beste Ergebnis der Simulationen?
- wie mit reward experimentieren dass man ihn ermutigt 16x16 auszuprobieren
- wie dynamisch mit dem switchen umgehen (zb nach jedem update mal probieren?)
- funzt der 16x16 richtig (oder kann was wegen int-rounding schief gehen ; bzw. warum wird es nie gewählt?
- Mehr


"""
5x5 aufteilen , 6x6 rausnehmen ; 4x4 mit 2x2 obs
- observation space reduzieren (komplexität des levels wird schwerer; und man kommt durch zufall leichter an) ; nutzlose iterationen sparen
    4, 6, 8, 10 (obs space macht es seeehr schwer)
"""


-------------------------------------------------------------------------------------
TODO ggf alte Models löschen, wenn man X epochs weiter ist. Solange man die relevanten logs noch hat

- Ist Speicherplatz eigentlich irgendwann ein Problem? die alten Models aus den Files könnte man ggf löschen, müsste dann aber noch mal sicherere gehen mit dem abspeichern
- welche performance metriken sollen überhaupt verwendet werden? Welche möchte man plotten später? (training vs eval reward, ...)

- Was für Env-Iterationen, Curriculum-Länge & -Anzahl werden überhaupt angestrebt?
- Kann man vllt so die Minigrid anpassen, dass ich nicht manuell auf register(...) in meinen inits zurückgreifen muss?
- Das Unlearning Problem: wie genau funktionieren die updates überhaupt (2560 update schritte, oder 1x pro level)
- Wie damit umgehen, dass man evtl nicht alles 1:1 gleich lang machen muss? zB Man kann 200k trainieren 16x16, und 100k 8x8 reicht zum stabilisieren
- RH Probleme? Was wenn man zB erst 2 Mio mal 16x16 trainieren muss, bevor man Fortschritte sieht? Wenn 1 Horizon bei 500k ist & die Iterationen pro Env noch niedriger sind
------------------------------------------

Paar Ideen
- Evaluation: use weights, maybe depending on progress ;  maybe use something else like Value / Policy Loss / Frames
# TODO maybe set biased pop for 1st epoch 1st generation like so: pop = Population.new("X", self.createFirstGeneration(self.curricula))


Small Refactorings
- use os . join for these things os.getcwd() + "\\storage\\" + directory)
- load time or set initially in linear / adaptive
- überall datetime einsetzen statt time



=========================================================================
MA Notizen

Vektor -> Matrix benennen (bzw. ggf 3d für die Repräsentation damit es übersichtlicher ist)

===========================================================================================

Changelog / Erkenntnisse
Reward nur am Ende ist super garbage




- 8x8, vor allem 16x16 wurde fast nie ausgewählt. Daher Reward anpassen, um ermutigen das zu tun
- envDifficulty (aka maxSteps) lässt reward etwas verzerrt aussehen; daher multiplier
- Difficulty Cutoffs: .25 / .75

17.05 - 24.05:
5x5 viewsize, dafür kleinere Level 4 7 9 12 (bzw 6 8 10 12) --> schicht entfernt im netz
-- Fix mit dem Tensorboard Bug
- bisschen history aufgeschrieben was wann warum passiert ist
- rewards normalisieren (damit man nicht verzerrt wird, ob es 3 oder 4 envs sind)
- Cuda broken ??
- Experiment: 4 Para trainieren Baseline  ( --> Reward instant bei 32 / 35); training mit pur 16x16 scheitert immer noch, paar h laufen lassen
- RRH Experiment (wollte eig mit neuen Envs testen ...) ; sehr jumpy ( --> colab)








-------------
Steps:
- penv speedup noch mal probieren (4-5h)
- cuda noch mal probieren ??
- evaluation (8h)
- anderen Minigrid umgebung als doorkey suchen / funktionsfähig machen
- neue Experimente: RHEA 2x2 Tage, RRH 1 Tage, Baseline 1 Tag